{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "***\n",
    "<a name=\"intro\"></a>\n",
    "## Introduction\n",
    "At the heart of neural networks is the *unit* (also called a *node* or *neuron*). A unit takes in one or more inputs, multiplies each input by a parameter (also called a *weight*), sums the weighted input’s values along with some bias value (typically 0), and then feeds the value into an activation function. This output is then sent forward to the other neurals deeper in the neural network (if they exist).\n",
    "\n",
    "*Feedforward* neural networks -- also called *multilayer perceptrons* -- are the simplest artificial neural network used in any real-world setting. A neural networks can be visualized as a series of connected layers that form a network connecting an observation’s feature values at one end, and the target value (e.g., observation’s class) at the other end. The name *feedforward* comes from the fact that an observation’s feature values are fed “forward” through the network, with each layer successively transforming the feature values with the goal that the output at the end is the same as the target’s value.\n",
    "\n",
    "![ANN](https://upload.wikimedia.org/wikipedia/commons/thumb/9/91/Multi-Layer_Neural_Network-Vector.svg/320px-Multi-Layer_Neural_Network-Vector.svg.png)\n",
    "\n",
    "Specifically, feedforward neural networks contain three types of layers of units. At the start of the neural network is an input layer where each unit contains an observation’s value for a single feature. For example, if an observation has 100 features, the input layer has 100 nodes. At the end of the neural network is the output layer, which transforms the output of the hidden layers into values useful for the task at hand. For example, if our goal was binary classification, we could use an output layer with a single unit that uses a sigmoid function to scale its own output to between 0 and 1, representing a predicted class probability. Between the input and output layers are the so-called “hidden” layers (which aren’t hidden at all). These hidden layers successively transform the feature values from the input layer to something that, once processed by the output layer, resembles the target class. Neural networks with many hidden layers (e.g., 10, 100, 1,000) are considered “deep” networks and their application is called *deep learning*.\n",
    "\n",
    "Neural networks are typically created with all parameters initialized as small random values from a Gaussian or normal uniform. Once an observation (or more often a set number of observations called a *batch*) is fed through the network, the outputted value is compared with the observation’s true value using a loss function. This is called *forward propagation*. Next an algorithm goes “backward” through the network identifying how much each parameter contributed to the error between the predicted and true values, a process called *backpropagation*. At each parameter, the optimization algorithm determines how much each weight should be adjusted to improve the output.\n",
    "\n",
    "Neural networks learn by repeating this process of forward propagation and backpropagation for every observation in the training data multiple times (each time all observations have been sent through the network is called an *epoch* and training typically consists of multiple epochs), iteratively updating the values of the parameters.\n",
    "\n",
    "In this guided exercise, we will use the popular Python library Keras to build, train, and evaluate a variety of neural networks. Keras is a high-level library, using other libraries like TensorFlow and Theano as its “engine.” For us the advantage of Keras is that we can focus on network design and training, leaving the specifics of the tensor operations to the other libraries.\n",
    "\n",
    "Neural networks created using Keras code can be trained using both CPUs (i.e., on your laptop) and GPUs (i.e., on a specialized deep learning computer). In the real world with real data, it is *highly* advisable to train neural networks using GPUs; however, for the sake of learning, all the neural networks in this guided exercise are small and simple enough to be trained on a laptop in only a few minutes. Just be aware that when we have larger networks and more training data, training using CPUs is *significantly* slower than training using GPUs.\n",
    "\n",
    "### See Also\n",
    "* [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/) (On-line textbook w/ guided exercises)\n",
    "\n",
    "***\n",
    "<a name=\"section-1\"></a>\n",
    "## 1. Preprocessing Data for Neural Networks\n",
    "\n",
    "### Problem\n",
    "\n",
    "You want to preprocess data for use in a neural network.\n",
    "\n",
    "### Solution\n",
    "\n",
    "Standardize each feature using scikit-learn’s `StandardScaler`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "# Create feature\n",
    "features = np.array([[-100.1, 3240.1],\n",
    "                     [-200.2, -234.1],\n",
    "                     [5000.5, 150.1],\n",
    "                     [6000.6, -125.1],\n",
    "                     [9000.9, -673.1]])\n",
    "\n",
    "# Create scaler\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "# Transform the feature\n",
    "features_standardized = scaler.fit_transform(features)\n",
    "\n",
    "# Show feature\n",
    "features_standardized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "While we covered this in a previous exercise, it is worth repeating because of how important it is for neural networks. Typically, a neural network’s parameters are initialized (i.e., created) as small random numbers. Neural networks often behave poorly when the feature values are much larger than parameter values. Furthermore, since an observation’s feature values are combined as they pass through individual units, it is important that all features have the same scale.\n",
    "\n",
    "For these reasons, it is best practice (although not always necessary; for example, when we have all binary features) to standardize each feature such that the feature’s values have the mean of 0 and the standard deviation of 1. This can be easily accomplished with scikit-learn’s `StandardScaler`.\n",
    "\n",
    "You can see the effect of the standardization by checking the mean and standard deviation of our first features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print mean and standard deviation\n",
    "print('Mean:', round(features_standardized[:,0].mean()))\n",
    "print('Standard deviation:', features_standardized[:,0].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  ** NOTE: Before continuing, install `Keras` **\n",
    "\n",
    "[Installation notes](https://anaconda.org/conda-forge/keras)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a name=\"section-2\"></a>\n",
    "## 2. Designing a Neural Network\n",
    "\n",
    "### Problem\n",
    "\n",
    "You want to design a neural network.\n",
    "\n",
    "### Solution\n",
    "\n",
    "Use Keras’ `Sequential` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "# Start neural network\n",
    "network = models.Sequential()\n",
    "\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=16, activation=\"relu\", input_shape=(10,)))\n",
    "\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "\n",
    "# Add fully connected layer with a sigmoid activation function\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile neural network\n",
    "network.compile(loss=\"binary_crossentropy\", # Cross-entropy\n",
    "                optimizer=\"rmsprop\", # Root Mean Square Propagation\n",
    "                metrics=[\"accuracy\"]) # Accuracy performance metric\n",
    "\n",
    "network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "Neural networks consist of layers of units. However, there is incredible variety in the types of layers and how they are combined to form the network’s architecture. At present, while there are commonly used architecture patterns (which we will cover in this exercise), the truth is that selecting the right architecture is mostly an art and the topic of much research.\n",
    "\n",
    "To construct a feedforward neural network in Keras, we need to make a number of choices about both the network architecture and training process. Remember that each unit in the hidden layers:\n",
    "\n",
    "1. Receives a number of inputs.\n",
    "\n",
    "2. Weights each input by a parameter value.\n",
    "\n",
    "3. Sums together all weighted inputs along with some bias (typically 0).\n",
    "\n",
    "4. Most often then applies some function (called an *activation function*).\n",
    "\n",
    "5. Sends the output on to units in the next layer.\n",
    "\n",
    "First, for each layer in the hidden and output layers we must define the number of units to include in the layer and the [activation function](https://en.wikipedia.org/wiki/Activation_function). Overall, the more units we have in a layer, the more our network is able to learn complex patterns. However, more units might make our network overfit the training data in a way detrimental to the performance on the test data.\n",
    "\n",
    "For hidden layers, a popular activation function is the *rectified linear unit* (ReLU):\n",
    "\n",
    "$$f(z)=\\max(0,z)$$\n",
    "\n",
    "where $z$ is the sum of the weighted inputs and bias. As we can see, if $z$ is greater than 0, the activation function returns $z$; otherwise, the function returns 0. \n",
    "\n",
    "<br/><br/> \n",
    "![ReLU](https://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Activation_rectified_linear.svg/480px-Activation_rectified_linear.svg.png)\n",
    "<br/><br/> \n",
    "\n",
    "This simple activation function has a number of desirable properties (a discussion of which is beyond the scope of this exercise) and this has made it a popular choice in neural networks. We should be aware, however, that many dozens of activation functions exist.\n",
    "\n",
    "Second, we need to define the number of hidden layers to use in the network. More layers allow the network to learn more complex relationships, but with a computational cost.\n",
    "\n",
    "Third, we have to define the structure of the activation function (if any) of the output layer. The nature of the output function is often determined by the goal of the network. Here are some common output layer patterns:\n",
    "\n",
    "**Binary classification**\n",
    "\n",
    "\n",
    "One unit with a sigmoid activation function.\n",
    "\n",
    "\n",
    "**Multiclass classification**\n",
    "\n",
    "\n",
    "$k$ units (where $k$ is the number of target classes) and a softmax activation function.\n",
    "\n",
    "\n",
    "**Regression**  \n",
    "\n",
    "One unit with no activation function.\n",
    "    \n",
    "Fourth, we need to define a loss function (the function that measures how well a predicted value matches the true value); this is again often determined by the problem type:\n",
    "\n",
    "\n",
    "**Binary classification**  \n",
    "\n",
    "\n",
    "Binary cross-entropy.\n",
    "\n",
    "\n",
    "**Multiclass classification**  \n",
    "\n",
    "\n",
    "Categorical cross-entropy.\n",
    "\n",
    "**Regression** \n",
    "\n",
    "\n",
    "Mean square error.\n",
    "\n",
    "\n",
    "Fifth, we have to define an optimizer, which intuitively can be thought of as our strategy “walking around” the loss function to find the parameter values that produce the lowest error. Common choices for optimizers are stochastic gradient descent, stochastic gradient descent with momentum, root mean square propagation, and adaptive moment estimation (more information on these optimizers in \"[See Also](#SeeAlso))\".\n",
    "\n",
    "Sixth, we can select one or more metrics to use to evaluate the performance, such as accuracy.\n",
    "\n",
    "Keras offers two ways for creating neural networks. Keras’ sequential model creates neural networks by stacking together layers. An alternative method for creating neural networks is called the [functional API](https://keras.io/api/), but that is more for researchers rather than practitioners.\n",
    "\n",
    "In our solution, we created a two-layer neural network (when counting layers we don’t include the input layer because it does not have any parameters to learn) using Keras’ sequential model. Each layer is “dense” (also called fully connected), meaning that all the units in the previous layer are connected to all the neurals in the next layer. In the first hidden layer we set `units=16`, meaning that layer contains 16 units with ReLU activation functions: `activation='relu'`. In Keras, the first hidden layer of any network has to include an `input_shape` parameter, which is the shape of feature data. For example, (10,) tells the first layer to expect each observation to have 10 feature values. Our second layer is the same as the first, without the need for the `input_shape` parameter. This network is designed for binary classification so the output layer contains only one unit with a sigmoid activation function, which constrains the output to between 0 and 1 (representing the probability an observation is class 1).\n",
    "\n",
    "Finally, before we can train our model, we need to tell Keras how we want our network to learn. We do this using the `compile` method, with our optimization algorithm (`RMSProp`), loss function (`binary_crossentropy`), and one or more performance metrics.\n",
    "\n",
    "<a name=\"SeeAlso\"></a>\n",
    "### See Also\n",
    "<a name=\"activation\"></a>\n",
    "* [Activation function, Wikipedia](https://en.wikipedia.org/wiki/Activation_function)    \n",
    "<a name=\"losses\"></a>    \n",
    "* [Losses, Keras](https://keras.io/losses/)\n",
    "* [Loss functions for classification, Wikipedia](https://en.wikipedia.org/wiki/Loss_functions_for_classification)\n",
    "* [On Loss Functions for Deep Neural Networks in Classification](https://arxiv.org/abs/1702.05659)\n",
    "\n",
    "***\n",
    "<a name=\"section-3\"></a>\n",
    "## 3. Training a Binary Classifier\n",
    "### Problem  \n",
    "You want to train a binary classifier neural network.\n",
    "\n",
    "### Solution  \n",
    "Use Keras to construct a feedforward neural network and train it using the `fit` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# Set the number of features we want\n",
    "number_of_features = 1000\n",
    "\n",
    "# Load data and target vector from movie review data\n",
    "(data_train, target_train), (data_test, target_test) = imdb.load_data(\n",
    "    num_words=number_of_features)\n",
    "\n",
    "# Convert movie review data to one-hot encoded feature matrix\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
    "features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")\n",
    "\n",
    "# Start neural network\n",
    "network = models.Sequential()\n",
    "\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=16, activation=\"relu\", input_shape=(\n",
    "    number_of_features,)))\n",
    "\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "\n",
    "# Add fully connected layer with a sigmoid activation function\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile neural network\n",
    "network.compile(loss=\"binary_crossentropy\", # Cross-entropy\n",
    "                optimizer=\"rmsprop\", # Root Mean Square Propagation\n",
    "                metrics=[\"accuracy\"]) # Accuracy performance metric\n",
    "\n",
    "# Train neural network\n",
    "history = network.fit(features_train, # Features\n",
    "                      target_train, # Target vector\n",
    "                      epochs=3, # Number of epochs\n",
    "                      verbose=1, # Print description after each epoch\n",
    "                      batch_size=100, # Number of observations per batch\n",
    "                      validation_data=(features_test, target_test)) # Test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### ** Note: ignore the warning \"Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated.\" **\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "In [Section 2](#section-2), we discussed how to construct a neural network using Keras’ `sequential` model. Here we train that neural network using real data. Specifically, we use 50,000 movie reviews (25,000 as training data, 25,000 held out for testing), categorized as positive or negative. We convert the text of the reviews into 1,000 binary features indicating the presence of one of the 1,000 most frequent words. Put more simply, our neural networks will use 25,000 observations, each with 1,000 features, to predict if a movie review is positive or negative.\n",
    "\n",
    "The neural network we are using is the same as the one in [Section 2](#section-2) (go there for a detailed explanation). The only addition is that in that implementation we only created the neural network, we didn’t train it.\n",
    "\n",
    "In Keras, we train our neural network using the `fit` method. There are six significant parameters to define. The first two parameters are the features and target vector of the training data. We can view the shape of feature matrix using `shape`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View shape of feature matrix\n",
    "features_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `epochs` parameter defines how many epochs to use when training the data. `verbose` determines how much information is outputted during the training process, with `0` being no output, `1` outputting a progress bar, and `2` one log line per epoch. `batch_size` sets the number of observations to propagate through the network before updating the parameters.\n",
    "\n",
    "Finally, we held out a test set of data to use to evaluate the model. These test features and test target vector can be arguments of `validation_data`, which will use them for evaluation. Alternatively, we could have used `validation_split` to define what fraction of the training data we want to hold out for evaluation.\n",
    "\n",
    "In scikit-learn the `fit` method returned a trained model, but in Keras the `fit` method returns a `History` object containing the loss values and performance metrics at each epoch.\n",
    "\n",
    "<a name=\"section-4\"></a>\n",
    "## 4. Training a Multiclass Classifier\n",
    "### Problem\n",
    "You want to train a multiclass classifier neural network.\n",
    "\n",
    "### Solution\n",
    "Use Keras to construct a feedforward neural network with an output layer with softmax activation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "from keras.datasets import reuters\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# Set the number of features we want\n",
    "number_of_features = 5000\n",
    "\n",
    "# Load feature and target data\n",
    "data = reuters.load_data(num_words=number_of_features)\n",
    "(data_train, target_vector_train), (data_test, target_vector_test) = data\n",
    "\n",
    "# Convert feature data to a one-hot encoded feature matrix\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
    "features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")\n",
    "\n",
    "# One-hot encode target vector to create a target matrix\n",
    "target_train = to_categorical(target_vector_train)\n",
    "target_test = to_categorical(target_vector_test)\n",
    "\n",
    "# Start neural network\n",
    "network = models.Sequential()\n",
    "\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=100,\n",
    "                         activation=\"relu\",\n",
    "                         input_shape=(number_of_features,)))\n",
    "\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=100, activation=\"relu\"))\n",
    "\n",
    "# Add fully connected layer with a softmax activation function\n",
    "network.add(layers.Dense(units=46, activation=\"softmax\"))\n",
    "\n",
    "# Compile neural network\n",
    "network.compile(loss=\"categorical_crossentropy\", # Cross-entropy\n",
    "                optimizer=\"rmsprop\", # Root Mean Square Propagation\n",
    "                metrics=[\"accuracy\"]) # Accuracy performance metric\n",
    "\n",
    "# Train neural network\n",
    "history = network.fit(features_train, # Features\n",
    "                      target_train, # Target\n",
    "                      epochs=3, # Three epochs\n",
    "                      verbose=0, # No output\n",
    "                      batch_size=100, # Number of observations per batch\n",
    "                      validation_data=(features_test, target_test)) # Test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "In this solution, we created a neural network similar to the binary classifier from the last section, but with some notable changes. First, our data is 11,228 Reuters newswires. Each newswire is categorized into 46 topics. We prepared our feature data by converting the newswires into 5,000 binary features (denoting the presence of a certain word in the newswires). We prepared the target data by one-hot encoding it so that we obtain a target matrix denoting which of the 46 classes an observation belongs to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View target matrix\n",
    "target_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we increased the number of units in each of the hidden layers to help the neural network represent the more complex relationship between the 46 classes.\n",
    "\n",
    "Third, since this is a *multiclass* classification problem, we used an output layer with 46 units (one per class) containing a softmax activation function. The softmax activation function will return an array of 46 values summing to 1. These 46 values represent an observation’s probability of being a member of each of the 46 classes.\n",
    "\n",
    "Fourth, we used a loss function suited to multiclass classification, the categorical cross-entropy loss function, ``categorical_crossentropy``.\n",
    "***\n",
    "<a name=\"section-5\"></a>\n",
    "## 5. Training a Regressor\n",
    "\n",
    "### Problem\n",
    "You want to train a neural network for regression.\n",
    "\n",
    "### Solution\n",
    "Use Keras to construct a feedforward neural network with a single output unit and no activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# Generate features matrix and target vector\n",
    "features, target = make_regression(n_samples = 10000,\n",
    "                                   n_features = 3,\n",
    "                                   n_informative = 3,\n",
    "                                   n_targets = 1,\n",
    "                                   noise = 0.0,\n",
    "                                   random_state = 0)\n",
    "\n",
    "# Divide our data into training and test sets\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "features, target, test_size=0.33, random_state=0)\n",
    "\n",
    "# Start neural network\n",
    "network = models.Sequential()\n",
    "\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=32,\n",
    "                         activation=\"relu\",\n",
    "                         input_shape=(features_train.shape[1],)))\n",
    "\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=32, activation=\"relu\"))\n",
    "\n",
    "# Add fully connected layer with no activation function\n",
    "network.add(layers.Dense(units=1))\n",
    "\n",
    "# Compile neural network\n",
    "network.compile(loss=\"mse\", # Mean squared error\n",
    "                optimizer=\"RMSprop\", # Optimization algorithm\n",
    "                metrics=[\"mse\"]) # Mean squared error\n",
    "\n",
    "# Train neural network\n",
    "history = network.fit(features_train, # Features\n",
    "                      target_train, # Target vector\n",
    "                      epochs=10, # Number of epochs\n",
    "                      verbose=0, # No output\n",
    "                      batch_size=100, # Number of observations per batch\n",
    "                      validation_data=(features_test, target_test)) # Test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "It is completely possible to create a neural network to predict continuous values instead of class probabilities. In the case of our binary classifier ([Section 3](#section-3)) we used an output layer with a single unit and a sigmoid activation function to produce a probability that an observation was class 1. Importantly, the sigmoid activation function constrained the outputted value to between 0 and 1. If we remove that constraint by having no activation function, we allow the output to be a continuous value.\n",
    "\n",
    "Furthermore, because we are training a regressor, we should use an appropriate loss function and evaluation metric, in our case the mean square error:\n",
    "\n",
    "$$MSE=\\frac{1}{n}\\sum_{i=1}^{n}(\\hat{y_i}-y_i)^2$$\n",
    "\n",
    "where $n$ is the number of observations; $y_i$ is the true value of the target we are trying to predict, $y$, for observation $i$; and $\\hat{y_i}$ is the model’s predicted value for $y_i$.\n",
    "\n",
    "Finally, because we are using simulated data using scikit-learn, ``make_regression``, we didn’t have to standardize the features. It should be noted, however, that in almost all real-world cases standardization would be necessary.\n",
    "\n",
    "***\n",
    "<a name=\"section-6\"></a>\n",
    "## 6. Making Predictions\n",
    "### Problem\n",
    "You want to use a neural network to make predictions.\n",
    "\n",
    "### Solution\n",
    "Use Keras to construct a feedforward neural network, then make predictions using ``predict``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# Set the number of features we want\n",
    "number_of_features = 10000\n",
    "\n",
    "# Load data and target vector from IMDB movie data\n",
    "(data_train, target_train), (data_test, target_test) = imdb.load_data(\n",
    "    num_words=number_of_features)\n",
    "\n",
    "# Convert IMDB data to a one-hot encoded feature matrix\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
    "features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")\n",
    "\n",
    "# Start neural network\n",
    "network = models.Sequential()\n",
    "\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=16,\n",
    "                         activation=\"relu\",\n",
    "                         input_shape=(number_of_features,)))\n",
    "\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "\n",
    "# Add fully connected layer with a sigmoid activation function\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile neural network\n",
    "network.compile(loss=\"binary_crossentropy\", # Cross-entropy\n",
    "                optimizer=\"rmsprop\", # Root Mean Square Propagation\n",
    "                metrics=[\"accuracy\"]) # Accuracy performance metric\n",
    "\n",
    "# Train neural network\n",
    "history = network.fit(features_train, # Features\n",
    "                      target_train, # Target vector\n",
    "                      epochs=3, # Number of epochs\n",
    "                      verbose=0, # No output\n",
    "                      batch_size=100, # Number of observations per batch\n",
    "                      validation_data=(features_test, target_test)) # Test data\n",
    "\n",
    "# Predict classes of test set\n",
    "predicted_target = network.predict(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "Making predictions is easy in Keras. Once we have trained our neural network we can use the ``predict`` method, which takes as an argument a set of features and returns the predicted output for each observation. In our solution, our neural network is set up for binary classification so the predicted output is the probability of being class 1. Observations with predicted values very close to 1 are highly likely to be class 1, while observations with predicted values very close to 0 are highly likely to be class 0. For example, this is the predicted probability that the first observation in our test feature matrix is class 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the probability the first observation is class 1\n",
    "predicted_target[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a name=\"section-7\"></a>\n",
    "## 7. Visualize Training History\n",
    "### Problem\n",
    "You want to find the “sweet spot” in a neural network’s loss and/or accuracy score.\n",
    "\n",
    "### Solution\n",
    "Use Matplotlib to visualize the loss of the test and training set over each epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# Set the number of features we want\n",
    "number_of_features = 10000\n",
    "\n",
    "# Load data and target vector from movie review data\n",
    "(data_train, target_train), (data_test, target_test) = imdb.load_data(\n",
    "    num_words=number_of_features)\n",
    "\n",
    "# Convert movie review data to a one-hot encoded feature matrix\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
    "features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")\n",
    "\n",
    "# Start neural network\n",
    "network = models.Sequential()\n",
    "\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=16,\n",
    "                         activation=\"relu\",\n",
    "                         input_shape=(number_of_features,)))\n",
    "\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "\n",
    "# Add fully connected layer with a sigmoid activation function\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile neural network\n",
    "network.compile(loss=\"binary_crossentropy\", # Cross-entropy\n",
    "                optimizer=\"rmsprop\", # Root Mean Square Propagation\n",
    "                metrics=[\"accuracy\"]) # Accuracy performance metric\n",
    "\n",
    "# Train neural network\n",
    "history = network.fit(features_train, # Features\n",
    "                      target_train, # Target\n",
    "                      epochs=15, # Number of epochs\n",
    "                      verbose=0, # No output\n",
    "                      batch_size=1000, # Number of observations per batch\n",
    "                      validation_data=(features_test, target_test)) # Test data\n",
    "\n",
    "# Get training and test loss histories\n",
    "training_loss = history.history[\"loss\"]\n",
    "test_loss = history.history[\"val_loss\"]\n",
    "\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Visualize loss history\n",
    "plt.plot(epoch_count, training_loss, \"r--\")\n",
    "plt.plot(epoch_count, test_loss, \"b-\")\n",
    "plt.legend([\"Training Loss\", \"Test Loss\"])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can use the same approach to visualize the training and test accuracy over each epoch:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and test accuracy histories\n",
    "training_accuracy = history.history[\"accuracy\"]\n",
    "test_accuracy = history.history[\"val_accuracy\"]\n",
    "plt.plot(epoch_count, training_accuracy, \"r--\")\n",
    "plt.plot(epoch_count, test_accuracy, \"b-\")\n",
    "\n",
    "# Visualize accuracy history\n",
    "plt.legend([\"Training Accuracy\", \"Test Accuracy\"])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy Score\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "When our neural network is new, it will have a poor performance. As the neural network learns on the training data, the model’s error on both the training and test set will tend to increase. However, at a certain point the neural network starts “memorizing” the training data, and overfits. When this starts happening, the training error will decrease while the test error will start increasing. Therefore, in many cases there is a “sweet spot” where the test error (which is the error we mainly care about) is at its lowest point. This effect can be plainly seen in the solution where we visualize the training and test loss at each epoch. Note that the test error is lowest around epoch five, after which the training loss continues to increase while the test loss starts increasing. At this point onward, the model is overfitting.\n",
    "***\n",
    "\n",
    "<a name=\"section-8\"></a>\n",
    "## 8. Reducing Overfitting with Weight Regularization\n",
    "### Problem\n",
    "You want to reduce overfitting.\n",
    "\n",
    "### Solution\n",
    "Try penalizing the parameters of the network, also called *weight regularization*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# Set the number of features we want\n",
    "number_of_features = 1000\n",
    "\n",
    "# Load data and target vector from movie review data\n",
    "(data_train, target_train), (data_test, target_test) = imdb.load_data(\n",
    "    num_words=number_of_features)\n",
    "\n",
    "# Convert movie review data to a one-hot encoded feature matrix\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
    "features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")\n",
    "\n",
    "# Start neural network\n",
    "network = models.Sequential()\n",
    "\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=16,\n",
    "                         activation=\"relu\",\n",
    "                         kernel_regularizer=regularizers.l2(0.01),\n",
    "                         input_shape=(number_of_features,)))\n",
    "\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=16,\n",
    "                         kernel_regularizer=regularizers.l2(0.01),\n",
    "                         activation=\"relu\"))\n",
    "\n",
    "# Add fully connected layer with a sigmoid activation function\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile neural network\n",
    "network.compile(loss=\"binary_crossentropy\", # Cross-entropy\n",
    "                optimizer=\"rmsprop\", # Root Mean Square Propagation\n",
    "                metrics=[\"accuracy\"]) # Accuracy performance metric\n",
    "\n",
    "# Train neural network\n",
    "history = network.fit(features_train, # Features\n",
    "                      target_train, # Target vector\n",
    "                      epochs=3, # Number of epochs\n",
    "                      verbose=1, # Print description after each epoch\n",
    "                      batch_size=100, # Number of observations per batch\n",
    "                      validation_data=(features_test, target_test)) # Test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "One strategy to combat overfitting neural networks is by penalizing the parameters (i.e., weights) of the neural network such that they are driven to be small values—creating a simpler model less prone to overfit. This method is called weight regularization or weight decay. More specifically, in weight regularization a penalty is added to the loss function, such as the $L_2$ norm.\n",
    "\n",
    "In Keras, we can add a weight regularization by including using ``kernel_regularizer=regularizers.l2(0.01)`` in a layer’s parameters. In this example, 0.01 determines how much we penalize higher parameter values.\n",
    "***\n",
    "\n",
    "<a name=\"section-9\"></a>\n",
    "## 9. Reducing Overfitting with Early Stopping\n",
    "### Problem\n",
    "You want to reduce overfitting.\n",
    "\n",
    "### Solution\n",
    "Try stopping training when the test loss stops decreasing, a strategy called *early stopping*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# Set the number of features we want\n",
    "number_of_features = 1000\n",
    "\n",
    "# Load data and target vector from movie review data\n",
    "(data_train, target_train), (data_test, target_test) = imdb.load_data(\n",
    "    num_words=number_of_features)\n",
    "\n",
    "# Convert movie review data to a one-hot encoded feature matrix\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
    "features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")\n",
    "\n",
    "# Start neural network\n",
    "network = models.Sequential()\n",
    "\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=16,\n",
    "                         activation=\"relu\",\n",
    "                         input_shape=(number_of_features,)))\n",
    "\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "\n",
    "# Add fully connected layer with a sigmoid activation function\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile neural network\n",
    "network.compile(loss=\"binary_crossentropy\", # Cross-entropy\n",
    "                optimizer=\"rmsprop\", # Root Mean Square Propagation\n",
    "                metrics=[\"accuracy\"]) # Accuracy performance metric\n",
    "\n",
    "# Set callback functions to early stop training and save the best model so far\n",
    "callbacks = [EarlyStopping(monitor=\"val_loss\", patience=2),\n",
    "             ModelCheckpoint(filepath=\"best_model.h5\",\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True)]\n",
    "\n",
    "# Train neural network\n",
    "history = network.fit(features_train, # Features\n",
    "                      target_train, # Target vector\n",
    "                      epochs=20, # Number of epochs\n",
    "                      callbacks=callbacks, # Early stopping\n",
    "                      verbose=1, # Print description after each epoch\n",
    "                      batch_size=100, # Number of observations per batch\n",
    "                      validation_data=(features_test, target_test)) # Test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "As we discussed in [Section 7](#section-7) on Visualizing Training History, typically in the first training epochs both the training and test errors will decrease, but at some point the network will start “memorizing” the training data, causing the training error to continue to decrease even while the test error starts increasing. Because of this phenomenon, one of the most common and very effective methods to counter overfitting is to monitor the training process and stop training when the test error starts to increase. This strategy is called early stopping.\n",
    "\n",
    "In Keras, we can implement early stopping as a callback function. Callbacks are functions that can be applied at certain stages of the training process, such as at the end of each epoch. Specifically, in our solution, we included `EarlyStopping(monitor='val_loss', patience=2)` to define that we wanted to monitor the test (validation) loss at each epoch and after the test loss has not improved after two epochs, training is interrupted. But, since we set `patience=2`, we won’t get the best model, but the model two epochs after the best model. Therefore, optionally, we can include a second operation, `ModelCheckpoint`, which saves the model to a file after every checkpoint (which can be useful in case a multiday training session is interrupted for some reason). It would be helpful for us, if we set `save_best_only=True`, because then `ModelCheckpoint` will only save the best model.\n",
    "\n",
    "---\n",
    "<a name=\"section-10\"></a>\n",
    "## 10. Reducing Overfitting with Dropout\n",
    "### Problem\n",
    "You want to reduce overfitting.\n",
    "\n",
    "### Solution\n",
    "Introduce noise into your network’s architecture using dropout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# Set the number of features we want\n",
    "number_of_features = 1000\n",
    "\n",
    "# Load data and target vector from movie review data\n",
    "(data_train, target_train), (data_test, target_test) = imdb.load_data(\n",
    "    num_words=number_of_features)\n",
    "\n",
    "# Convert movie review data to a one-hot encoded feature matrix\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
    "features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")\n",
    "\n",
    "# Start neural network\n",
    "network = models.Sequential()\n",
    "\n",
    "# Add a dropout layer for input layer\n",
    "network.add(layers.Dropout(0.2, input_shape=(number_of_features,)))\n",
    "\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "\n",
    "# Add a dropout layer for previous hidden layer\n",
    "network.add(layers.Dropout(0.5))\n",
    "\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "\n",
    "# Add a dropout layer for previous hidden layer\n",
    "network.add(layers.Dropout(0.5))\n",
    "\n",
    "# Add fully connected layer with a sigmoid activation function\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile neural network\n",
    "network.compile(loss=\"binary_crossentropy\", # Cross-entropy\n",
    "                optimizer=\"rmsprop\", # Root Mean Square Propagation\n",
    "                metrics=[\"accuracy\"]) # Accuracy performance metric\n",
    "\n",
    "# Train neural network\n",
    "history = network.fit(features_train, # Features\n",
    "                      target_train, # Target vector\n",
    "                      epochs=3, # Number of epochs\n",
    "                      verbose=1, # Print description after each epoch\n",
    "                      batch_size=100, # Number of observations per batch\n",
    "                      validation_data=(features_test, target_test)) # Test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Discussion\n",
    "\n",
    "Dropout is a popular and powerful method for regularizing neural networks. In dropout, every time a batch of observations is created for training, a proportion of the units in one or more layers is multiplied by zero (i.e., dropped). In this setting, every batch is trained on the same network (e.g., the same parameters), but each batch is confronted by a slightly different version of that network’s *architecture*.\n",
    "\n",
    "Dropout is effective because by constantly and randomly dropping units in each batch, it forces units to learn parameter values able to perform under a wide variety of network architectures. That is, they learn to be robust to disruptions (i.e., noise) in the other hidden units, and this prevents the network from simply memorizing the training data.\n",
    "\n",
    "It is possible to add dropout to both the hidden and input layers. When an input layer is dropped, its feature value is not introduced into the network for that batch. A common choice for the portion of units to drop is 0.2 for input units and 0.5 for hidden units.\n",
    "\n",
    "In Keras, we can implement dropout by adding `Dropout` layers into our network architecture. Each `Dropout` layer will drop a user-defined hyperparameter of units in the previous layer every batch. Remember in Keras the input layer is assumed to be the first layer and not added using `add`. Therefore, if we want to add dropout to the input layer, the first layer we add in our network architecture is a dropout layer. This layer contains both the proportion of the input layer’s units to drop 0.2 and `input_shape` defining the shape of the observation data. Next, we add a dropout layer with 0.5 after each of the hidden layers.\n",
    "***\n",
    "\n",
    "<a name=\"section-11\"></a>\n",
    "## 11. Saving Model Training Progress\n",
    "### Problem\n",
    "Given a neural network that will take a long time to train, you want to save your progress in case the training process is interrupted.\n",
    "\n",
    "### Solution\n",
    "Use the callback function ``ModelCheckpoint`` to save the model after every epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# Set the number of features we want\n",
    "number_of_features = 1000\n",
    "\n",
    "# Load data and target vector from movie review data\n",
    "(data_train, target_train), (data_test, target_test) = imdb.load_data(\n",
    "    num_words=number_of_features)\n",
    "\n",
    "# Convert movie review data to a one-hot encoded feature matrix\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
    "features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")\n",
    "\n",
    "# Start neural network\n",
    "network = models.Sequential()\n",
    "\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=16,\n",
    "                         activation=\"relu\",\n",
    "                         input_shape=(number_of_features,)))\n",
    "\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "\n",
    "# Add fully connected layer with a sigmoid activation function\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile neural network\n",
    "network.compile(loss=\"binary_crossentropy\", # Cross-entropy\n",
    "                optimizer=\"rmsprop\", # Root Mean Square Propagation\n",
    "                metrics=[\"accuracy\"]) # Accuracy performance metric\n",
    "\n",
    "# Set callback functions to early stop training and save the best model so far\n",
    "checkpoint = [ModelCheckpoint(filepath=\"models.hdf5\")]\n",
    "\n",
    "# Train neural network\n",
    "history = network.fit(features_train, # Features\n",
    "                      target_train, # Target vector\n",
    "                      epochs=3, # Number of epochs\n",
    "                      callbacks=checkpoint, # Checkpoint\n",
    "                      verbose=1, # Print description after each epoch\n",
    "                      batch_size=100, # Number of observations per batch\n",
    "                      validation_data=(features_test, target_test)) # Test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "In [Section 8](#section-8), we used the callback function ``ModelCheckpoint`` in conjunction with ``EarlyStopping`` to end monitoring and end training when the test error stopped improving. However, there is another, more mundane reason for using ``ModelCheckpoint``. In the real world, it is common for neural networks to train for hours or even days. During that time a lot can go wrong: computers can lose power, servers can crash, or inconsiderate graduate students can close your laptop.\n",
    "\n",
    "`ModelCheckpoint` alleviates this problem by saving the model after every epoch. Specifically, after every epoch `ModelCheckpoint` saves a model to the location specified by the filepath parameter. If we include only a filename (e.g., *models.hdf5*) that file will be overridden with the latest model every epoch. If we only wanted to save the best model according to the performance of some loss function, we can set `save_best_only=True` and `monitor='val_loss'` to not override a file if the model has a worse test loss than the previous model. Alternatively, we can save every epoch’s model as its own file by including the epoch number and test loss score into the filename itself. For example, if we set filepath to `model_{epoch:02d}_{val_loss:.2f}.hdf5`, the name of the file containing the model saved after the 11th epoch with a test loss value of 0.33 would be *model_10_0.35.hdf5* (notice that the epoch number is 0-indexed). \n",
    "*** \n",
    "<a name=\"section-12\"></a>\n",
    "## 12. $k$-Fold Cross-Validating Neural Networks\n",
    "### Problem\n",
    "You want to evaluate a neural network using $k$-fold cross-validation.\n",
    "\n",
    "### Solution\n",
    "Often $k$-fold cross-validating neural networks is neither necessary nor advisable. However, if it is appropriate: use Keras’ scikit-learn wrapper to allow Keras’ sequential models to use the scikit-learn API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# Number of features\n",
    "number_of_features = 100\n",
    "\n",
    "# Generate features matrix and target vector\n",
    "features, target = make_classification(n_samples = 10000,\n",
    "                                       n_features = number_of_features,\n",
    "                                       n_informative = 3,\n",
    "                                       n_redundant = 0,\n",
    "                                       n_classes = 2,\n",
    "                                       weights = [.5, .5],\n",
    "                                       random_state = 0)\n",
    "\n",
    "# Create function returning a compiled network\n",
    "def create_network():\n",
    "\n",
    "    # Start neural network\n",
    "    network = models.Sequential()\n",
    "\n",
    "    # Add fully connected layer with a ReLU activation function\n",
    "    network.add(layers.Dense(units=16, activation=\"relu\", input_shape=(\n",
    "        number_of_features,)))\n",
    "\n",
    "    # Add fully connected layer with a ReLU activation function\n",
    "    network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "\n",
    "    # Add fully connected layer with a sigmoid activation function\n",
    "    network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "    # Compile neural network\n",
    "    network.compile(loss=\"binary_crossentropy\", # Cross-entropy\n",
    "                    optimizer=\"rmsprop\", # Root Mean Square Propagation\n",
    "                    metrics=[\"accuracy\"]) # Accuracy performance metric\n",
    "\n",
    "    # Return compiled network\n",
    "    return network\n",
    "\n",
    "# Wrap Keras model so it can be used by scikit-learn\n",
    "neural_network = KerasClassifier(build_fn=create_network,\n",
    "                                 epochs=10,\n",
    "                                 batch_size=100,\n",
    "                                 verbose=1)\n",
    "\n",
    "# Evaluate neural network using three-fold cross-validation\n",
    "cross_val_score(neural_network, features, target, cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "Theoretically, there is no reason we cannot use cross-validation to evaluate neural networks. However, neural networks are often used on very large data and can take hours or even days to train. For this reason, if the training time is long, adding the computational expense of $k$-fold cross-validation is unadvisable. For example, a model normally taking one day to train would take 10 days to evaluate using 10-fold cross-validation. If we have large data, it is often appropriate to simply evaluate the neural network on some test set.\n",
    "\n",
    "If we have smaller data, k-fold cross-validation can be useful to maximize our ability to evaluate the neural network’s performance. This is possible in Keras because we can “wrap” any neural network such that it can use the evaluation features available in scikit-learn, including $k$-fold cross-validation. To accomplish this, we first have to create a function that returns a compiled neural network. Next we use `KerasClassifier` (if we have a classifier; if we have a regressor we can use `KerasRegressor`) to wrap the model so it can be used by scikit-learn. After this, we can use our neural network like any other scikit-learn learning algorithm (e.g., random forests, logistic regression). In our solution, we used `cross_val_score` to run a three-fold cross-validation on our neural network.\n",
    "***\n",
    "\n",
    "<a name=\"section-13\"></a>\n",
    "## 13. Tuning Neural Networks\n",
    "### Problem\n",
    "You want to automatically select the best hyperparameters for your neural network.\n",
    "\n",
    "### Solution\n",
    "Combine a Keras neural network with scikit-learn’s model selection tools like ``GridSearchCV``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# Number of features\n",
    "number_of_features = 100\n",
    "\n",
    "# Generate features matrix and target vector\n",
    "features, target = make_classification(n_samples = 10000,\n",
    "                                       n_features = number_of_features,\n",
    "                                       n_informative = 3,\n",
    "                                       n_redundant = 0,\n",
    "                                       n_classes = 2,\n",
    "                                       weights = [.5, .5],\n",
    "                                       random_state = 0)\n",
    "\n",
    "# Create function returning a compiled network\n",
    "def create_network(optimizer=\"rmsprop\"):\n",
    "\n",
    "    # Start neural network\n",
    "    network = models.Sequential()\n",
    "\n",
    "    # Add fully connected layer with a ReLU activation function\n",
    "    network.add(layers.Dense(units=16,\n",
    "                             activation=\"relu\",\n",
    "                             input_shape=(number_of_features,)))\n",
    "\n",
    "    # Add fully connected layer with a ReLU activation function\n",
    "    network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "\n",
    "    # Add fully connected layer with a sigmoid activation function\n",
    "    network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "    # Compile neural network\n",
    "    network.compile(loss=\"binary_crossentropy\", # Cross-entropy\n",
    "                    optimizer=optimizer, # Optimizer\n",
    "                    metrics=[\"accuracy\"]) # Accuracy performance metric\n",
    "\n",
    "    # Return compiled network\n",
    "    return network\n",
    "\n",
    "# Wrap Keras model so it can be used by scikit-learn\n",
    "neural_network = KerasClassifier(build_fn=create_network, verbose=0)\n",
    "\n",
    "# Create hyperparameter space\n",
    "epochs = [5, 10]\n",
    "batches = [5, 10, 100]\n",
    "optimizers = [\"rmsprop\", \"adam\"]\n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(optimizer=optimizers, epochs=epochs, batch_size=batches)\n",
    "\n",
    "# Create grid search\n",
    "grid = GridSearchCV(estimator=neural_network, param_grid=hyperparameters)\n",
    "\n",
    "# Fit grid search\n",
    "grid_result = grid.fit(features, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "In this section we want to identify the best hyperparameters of a neural network.  The hyperparameters of a model are important and should be selected with care. However, before we get it into our heads that model selection strategies like grid search are a good idea, we must realize that if our model would normally have taken 12 hours or a day to train, this grid search process could take a week or more. Therefore, automatic hyperparameter tuning of neural networks is not the silver bullet, but it is a useful tool to have in certain circumstances.\n",
    "\n",
    "In our solution we conducted a cross-validated grid search over a number of options for the optimization algorithm, number of epochs, and batch size. Even this toy example took a few minutes to run, but once it is done we can use ``best_params_`` to view the hyperparameters of the neural network with the best results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View hyperparameters of best neural network\n",
    "grid_result.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a name=\"section-14\"></a>\n",
    "## 14. Visualizing Neural Networks\n",
    "\n",
    "### Problem\n",
    "You want to quickly visualize a neural network’s architecture.\n",
    "\n",
    "### Solution\n",
    "Use Keras’ ``model_to_dot`` or ``plot_model``:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: You may need to install:\n",
    "* `graphviz`\n",
    "* `pydot`\n",
    "* `pydotplus`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "\n",
    "# Start neural network\n",
    "network = models.Sequential()\n",
    "\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=16, activation=\"relu\", input_shape=(10,)))\n",
    "\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "\n",
    "# Add fully connected layer with a sigmoid activation function\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Visualize network architecture\n",
    "SVG(model_to_dot(network, show_shapes=True, show_layer_names=True, dpi = 65).create(prog=\"dot\", format=\"svg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, if we want to save the visualization as a file, we can use `plot_model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the visualization as a file\n",
    "plot_model(network, show_shapes=True, to_file=\"network.png\")  # file in current directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "Keras provides utility functions to quickly visualize neural networks. If we wish to display a neural network in a Jupyter Notebook, we can use `model_to_dot`. The `show_shapes` parameter shows the shape of the inputs and outputs and can help with debugging. For a simpler model, we can set `show_shapes=False`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize network architecture\n",
    "SVG(model_to_dot(network, show_shapes=False, dpi = 65).create(prog=\"dot\", format=\"svg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a name=\"section-15\"></a>\n",
    "## 15. Classifying Images\n",
    "### Problem\n",
    "You want to classify images using a convolutional neural network.\n",
    "\n",
    "### Solution\n",
    "Use Keras to create a neural network with at least one convolutional layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "\n",
    "# Set that the color channel value will be last\n",
    "K.set_image_data_format(\"channels_last\")\n",
    "\n",
    "# Set seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# Set image information\n",
    "channels = 1\n",
    "height = 28\n",
    "width = 28\n",
    "\n",
    "# Load data and target from MNIST data\n",
    "(data_train, target_train), (data_test, target_test) = mnist.load_data()\n",
    "\n",
    "# Reshape training image data into features\n",
    "data_train = data_train.reshape(data_train.shape[0], height, width, channels)\n",
    "\n",
    "# Reshape test image data into features\n",
    "data_test = data_test.reshape(data_test.shape[0], height, width, channels)\n",
    "\n",
    "\n",
    "# Rescale pixel intensity to between 0 and 1\n",
    "features_train = data_train / 255\n",
    "features_test = data_test / 255\n",
    "\n",
    "# One-hot encode target\n",
    "target_train = np_utils.to_categorical(target_train)\n",
    "target_test = np_utils.to_categorical(target_test)\n",
    "number_of_classes = target_test.shape[1]\n",
    "\n",
    "# Start neural network\n",
    "network = Sequential()\n",
    "\n",
    "# Add convolutional layer with 64 filters, a 5x5 window, and ReLU activation function\n",
    "network.add(Conv2D(filters=64,\n",
    "                   kernel_size=(5, 5),\n",
    "                   input_shape=(height, width, channels),\n",
    "                   activation='relu'))\n",
    "\n",
    "# Add max pooling layer with a 2x2 window\n",
    "network.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Add dropout layer\n",
    "network.add(Dropout(0.5))\n",
    "\n",
    "# Add layer to flatten input\n",
    "network.add(Flatten())\n",
    "\n",
    "# Add fully connected layer of 128 units with a ReLU activation function\n",
    "network.add(Dense(128, activation=\"relu\"))\n",
    "\n",
    "# Add dropout layer\n",
    "network.add(Dropout(0.5))\n",
    "\n",
    "# Add fully connected layer with a softmax activation function\n",
    "network.add(Dense(number_of_classes, activation=\"softmax\"))\n",
    "\n",
    "# Compile neural network\n",
    "network.compile(loss=\"categorical_crossentropy\", # Cross-entropy\n",
    "                optimizer=\"rmsprop\", # Root Mean Square Propagation\n",
    "                metrics=[\"accuracy\"]) # Accuracy performance metric\n",
    "\n",
    "# Train neural network\n",
    "network.fit(features_train, # Features\n",
    "            target_train, # Target\n",
    "            epochs=2, # Number of epochs\n",
    "            verbose=1, # Print description after each epoch\n",
    "            batch_size=1000, # Number of observations per batch\n",
    "            validation_data=(features_test, target_test)) # Data for evaluation\n",
    "\n",
    "# Visualize network architecture\n",
    "SVG(model_to_dot(network, show_shapes=True, show_layer_names=True, dpi = 65).create(prog=\"dot\", format=\"svg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "*Convolutional* neural networks (also called ConvNets) are a popular type of network that has proven very effective at computer vision (e.g., recognizing cats, dogs, planes, and even hot dogs). It is completely possible to use feedforward neural networks on images, where each pixel is a feature. However, when doing so we run into two major problems. First, feedforward neural networks do not take into account the spatial structure of the pixels. For example, in a 10\t× 10 pixel image we might convert it into a vector of 100 pixel features, and in this case feedforward would consider the first feature (e.g., pixel value) to have the same relationship with the 10th feature as the 11th feature. However, in reality the 10th feature represents a pixel on the far side of the image as the first feature, while the 11th feature represents the pixel immediately below the first pixel. Second, and relatedly, feedforward neural networks learn global relationships in the features instead of local patterns. In more practical terms, this means that feedforward neural networks are not able to detect an object regardless of where it appears in an image. For example, imagine we are training a neural network to recognize faces, and these faces might appear anywhere in the image from the upper right to the middle to the lower left.\n",
    "\n",
    "The power of convolutional neural networks is their ability handle both of these issues (and others). A complete explanation of convolutional neural networks is beyond our scope, but a brief explanation will be useful. The data of an individual image contains two or three dimensions: height, width, and depth. The first two should be obvious, but the last deserves explanation. The depth is the color of a pixel. In grayscale images the depth is only one (the intensity of the pixel) and therefore the image is a matrix. However, in color images a pixel’s color is represented by multiple values. For example, in an RGB image a pixel’s color is represented by three values representing red, green, and blue. Therefore, the data for an image can be imagined to be a three-dimensional tensor: width $\\times$ height $\\times$ depth (called feature maps). In convolutional neural networks, a convolution (don’t worry if you don’t know what that means) can be imagined to slide a window over the pixels of an image, looking at both the individual pixel and also its neighbors. It then transforms the raw image data into a new three-dimensional tensor where the first two dimensions are approximately width and height, while the third dimension (which contained the color values) now represents the patterns—called filters—(for example, a sharp corner or sweeping gradient) to which that pixel “belongs.”\n",
    "\n",
    "The second important concept for our purposes is that of pooling layers. Pooling layers move a window over our data (though usually they only look at every nth pixel, called striding) downsizing our data by summarizing the window in some way. The most common method is max pooling where the maximum value in each window is sent to the next layer. One reason for max pooling is merely practical; the convolutional process creates many more parameters to learn, which can get ungainly very quickly. Second, more intuitively, max pooling can be thought of as “zooming out” of an image.\n",
    "\n",
    "An example might be useful here. Imagine we have an image containing a dog’s face. The first convolutional layer might find patterns like shape edges. Then we use a max pool layer to “zoom out,” and a second convolutional layer to find patterns like dog ears. Finally we use another max pooling layer to zoom out again and a final convolutional layer to find patterns like dogs’ faces.\n",
    "\n",
    "Finally, fully connected layers are often used at the end of the network to do the actual classification.\n",
    "\n",
    "While our solution might look like a lot of lines of code, it is actually very similar to our binary classifier from earlier in this assignment. In this solution we used the famous MNIST dataset, which is a *de facto* benchmark dataset in machine learning. The MNIST dataset contains 70,000 small images (28 $\\times$ 28) of handwritten digits from 0 to 9. This dataset is labeled so that we know the actual digit (i.e., class) for each small image. The standard training-test split is to use 60,000 images for training and 10,000 for testing.\n",
    "\n",
    "We reorganized the data into the format expected by a convolutional network. Specifically, we used reshape to convert the observation data such that it is the shape Keras expects. Next, we rescaled the values to be between 0 and 1, since training performance can suffer if an observation’s values are much greater than the network’s parameters (which are initialized as small numbers). Finally, we one-hot encoded the target data so that every observation’s target has 10 classes, representing the digits 0–9.\n",
    "\n",
    "With the image data process we can build our convolutional network. First, we add a convolutional layer and specify the number of filters and other characteristics. The size of the window is a hyperparameter; however, 3\t$\\times$ 3 is standard practice for most images, while larger windows are often used in larger images. Second, we add a max pooling layer, summarizing the nearby pixels. Third, we add a dropout layer to reduce the chances of overfitting. Fourth, we add a flatten layer to convert the convolutionary inputs into a format able to be used by a fully connected layer. Finally, we add the fully connected layers and an output layer to do the actual classification. Notice that because this is a multiclass classification problem, we use the softmax activation function in the output layer.\n",
    "\n",
    "It should be noted that this is a pretty simple convolutional neural network. It is common to see a much deeper network with many more convolutional and max pooling layers stacked together.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "***\n",
    "<a name=\"section-16\"></a>\n",
    "## 16. Improving Performance with Image Augmentation\n",
    "\n",
    "### Note: Make sure you download the supporting images and set the correct path to them.\n",
    "\n",
    "### Problem\n",
    "\n",
    "You want to improve the performance of your convolutional neural network.\n",
    "### Solution\n",
    "\n",
    "For better results, preprocess the images and augment the data beforehand using `ImageDataGenerator`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Create image augmentation\n",
    "augmentation = ImageDataGenerator(featurewise_center=True, # Apply ZCA whitening\n",
    "                                  zoom_range=0.3, # Randomly zoom in on images\n",
    "                                  width_shift_range=0.2, # Randomly shift images\n",
    "                                  horizontal_flip=True, # Randomly flip images\n",
    "                                  rotation_range=90) # Randomly rotate\n",
    "\n",
    "# Process all images from the directory 'raw/images'\n",
    "augment_images = augmentation.flow_from_directory(\"raw/images\", # Image folder\n",
    "                                                  target_size=(28, 28), # Target size\n",
    "                                                  batch_size=32, # Batch size\n",
    "                                                  class_mode=\"binary\", # Classes\n",
    "                                                  save_to_dir=\"processed/images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "One way to improve the performance of a convolutional neural network is to preprocess the images. It is worth noting that Keras’ `ImageDataGenerator` contains a number of basic preprocessing techniques. For example, in our solution we used `featurewise_center=True` to standardize the pixels across the entire data.\n",
    "\n",
    "A second technique to improve performance is to add noise. An interesting feature of neural networks is that, counterintuitively, their performance often improves when noise is added to the data. The reason is that the additional noise can make the neural networks more robust in the face of real-world noise and prevents them from overfitting the data.\n",
    "\n",
    "When training convolutional neural networks for images, we can add noise to our observations by randomly transforming images in various ways, such as flipping images or zooming in on images. Even small changes can significantly improve model performance. We can use the same `ImageDataGenerator` class to conduct these transformations. The Keras documentation (referenced in [See Also](#ImageP)) specifies the full list of available transformations; however, our example contains a sample of them are, including random zooming, shifting, flipping, and rotation.\n",
    "\n",
    "It is important to note that the output of `flow_from_directory` is a Python generator object. This is because in most cases we will want to process the images on-demand as they are sent to the neural network for training. If we wanted to process all the images prior to training, we could simply iterate over the generator.\n",
    "\n",
    "Finally, since `augment_images` is a generator, when training our neural network we will have to use `fit_generator` instead of `fit`. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# not implemented, for example purposes\n",
    "\n",
    "# Train neural network\n",
    "#network.fit_generator(augment_images,\n",
    "#                      steps_per_epoch=2000,#Number of times to call the generator for each epoch\n",
    "#                      epochs=5,# Number of epochs\n",
    "#                      validation_data=augment_images_test, # Test data generator, not implemented\n",
    "#                      validation_steps=800) # Number of items to call the generator\n",
    "                                            # for each test epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See Also\n",
    "<a name=\"ImageP\"></a>\n",
    "* [Image Preprocessing, Keras](https://keras.io/api/preprocessing/image/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<a name=\"section-17\"></a>\n",
    "## 17. Classifying Text\n",
    "### Problem\n",
    "You want to classify text data.\n",
    "\n",
    "### Solution\n",
    "Use a long short-term memory recurrent neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load libraries\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# Set the number of features we want\n",
    "number_of_features = 1000\n",
    "\n",
    "# Load data and target vector from movie review data\n",
    "(data_train, target_train), (data_test, target_test) = imdb.load_data(\n",
    "    num_words=number_of_features)\n",
    "\n",
    "# Use padding or truncation to make each observation have 400 features\n",
    "features_train = sequence.pad_sequences(data_train, maxlen=400)\n",
    "features_test = sequence.pad_sequences(data_test, maxlen=400)\n",
    "\n",
    "# Start neural network\n",
    "network = models.Sequential()\n",
    "\n",
    "# Add an embedding layer\n",
    "network.add(layers.Embedding(input_dim=number_of_features, output_dim=128))\n",
    "\n",
    "# Add a long short-term memory layer with 128 units\n",
    "network.add(layers.LSTM(units=128))\n",
    "\n",
    "# Add fully connected layer with a sigmoid activation function\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile neural network\n",
    "network.compile(loss=\"binary_crossentropy\", # Cross-entropy\n",
    "                optimizer=\"Adam\", # Adam optimization\n",
    "                metrics=[\"accuracy\"]) # Accuracy performance metric\n",
    "\n",
    "# Train neural network\n",
    "history = network.fit(features_train, # Features\n",
    "                      target_train, # Target\n",
    "                      epochs=3, # Number of epochs\n",
    "                      verbose=1, # Print description after each epoch\n",
    "                      batch_size=1000, # Number of observations per batch\n",
    "                      validation_data=(features_test, target_test)) # Test data\n",
    "\n",
    "# Visualize network architecture\n",
    "SVG(model_to_dot(network, show_shapes=True, show_layer_names=True, dpi = 65).create(prog=\"dot\", format=\"svg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "Oftentimes we have text data that we want to classify. While it is possible to use a type of convolutional network, we are going to focus on a more popular option: the recurrent neural network. The key feature of recurrent neural networks is that information loops back in the network. This gives recurrent neural networks a type of memory they can use to better understand sequential data. A popular type of recurrent neural network is the long short-term memory (LSTM) network, which allows for information to loop backward in the network. For a more detailed explanation, see the [additional resources](#section-17-See-Also).\n",
    "\n",
    "In this solution, we have our movie review data from before and we want to train an LSTM network to predict if these reviews are positive or negative. Before we can train our network, a little data processing is needed. Our text data comes in the form of a list of integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View first observation\n",
    "print(data_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each integer in this list corresponds to some word. However, because each review does not contain the same number of words, each observation is not the same length. Therefore, before we can input this data into our neural network, we need to make all the observations the same length. We can do this using `pad_sequences`. `pad_sequences` pads each observation’s data so that they are all the same size. We can see this if we look at our first observation after it is processed by `pad_sequences`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View first observation\n",
    "print(features_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use one of the most promising techniques in natural language processing: word embeddings. In our binary classifier from [Section 3](#section-3), we one-hot encoded the observations and used that as inputs to the neural network. However, this time we will represent each word as a vector in a multidimensional space, and allow the distance between two vectors to represent the similarity between words. In Keras we can do this by adding an Embedding layer. For each value sent to the Embedding layer, it will output a vector representing that word. The following layer is our LSTM layer with 128 units, which allows for information from earlier inputs to be used in futures, directly addressing the sequential nature of the data. Finally, because this is a binary classification problem (each review is positive or negative), we add a fully connected output layer with one unit and a sigmoid activation function.\n",
    "\n",
    "It is worth noting that LSTMs are a very broad topic and the focus of much research. This approach, while hopefully useful, is far from the last word on the implementation of LSTMs.\n",
    "\n",
    "<a name=\"section-17-See-Also\"></a>\n",
    "### See Also\n",
    "* [Understanding LSTMs](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "*[3 Ways to Encode Categorical Variables for Deep Learning](https://machinelearningmastery.com/how-to-prepare-categorical-data-for-deep-learning-in-python/) (See Section on Learned Embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. References\n",
    "* [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/)\n",
    "* [Neural Networks, Manifolds, and Topology](https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/)\n",
    "* [Why Momentum Really Works](https://distill.pub/2017/momentum/)\n",
    "* [Understanding deep learning requires rethinking generalization](https://arxiv.org/abs/1611.03530)\n",
    "* [A Closer Look at Memorization in Deep Networks](https://arxiv.org/abs/1706.05394)\n",
    "* [The Loss Surfaces of Multilayer Networks](https://arxiv.org/abs/1412.0233)\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
