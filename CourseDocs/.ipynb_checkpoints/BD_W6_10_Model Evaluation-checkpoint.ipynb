{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This week we will examine strategies for evaluating the quality of models created through our learning algorithms.   Models are only as useful as the quality of their predictions, and thus fundamentally our goal is not to create models (which is easy) but to create high-quality models (which is hard). Therefore, as we explore the myriad learning algorithms that exist, we discuss how we can evaluate the models they produce.\n",
    "\n",
    "## 1. Cross-Validating Models\n",
    "\n",
    "### Problem\n",
    "\n",
    "You want to evaluate how well your model will work in the real world.\n",
    "### Solution\n",
    "\n",
    "Create a pipeline that preprocesses the data, trains the model, and then evaluates it using cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load digits dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# Create features matrix\n",
    "features = digits.data\n",
    "\n",
    "# Create target vector\n",
    "target = digits.target\n",
    "\n",
    "# Create standardizer\n",
    "standardizer = StandardScaler()\n",
    "\n",
    "# Create logistic regression object\n",
    "logit = LogisticRegression()\n",
    "\n",
    "# Create a pipeline that standardizes, then runs logistic regression\n",
    "pipeline = make_pipeline(standardizer, logit)\n",
    "\n",
    "# Create k-Fold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "# Conduct k-fold cross-validation\n",
    "cv_results = cross_val_score(pipeline, # Pipeline\n",
    "                             features, # Feature matrix\n",
    "                             target, # Target vector\n",
    "                             cv=kf, # Performance metric\n",
    "                             scoring=\"accuracy\", # Loss function\n",
    "                             n_jobs=-1) # Use all CPU cores\n",
    "\n",
    "# Calculate mean\n",
    "cv_results.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "At first consideration, evaluating supervised-learning models might appear straightforward: train a model and then calculate how well it did using some performance metric (accuracy, squared errors, etc.). However, this approach is fundamentally flawed. If we train a model using our data, and then evaluate how well it did on that data, we are not achieving our desired goal. Our goal is not to evaluate how well the model does on our training data, but how well it does on data it has never seen before (e.g., a new customer, a new crime, a new image). For this reason, our method of evaluation should help us understand how well models are able to make predictions from data they have never seen before.\n",
    "\n",
    "One strategy might be to hold off a slice of data for testing. This is called *validation* (or *hold-out*). In validation our observations (features and targets) are split into two sets, traditionally called the training set and the test set. We take the test set and put it off to the side, pretending that we have never seen it before. Next we train our model using our training set, using the features and target vector to teach the model how to make the best prediction. Finally, we simulate having never before seen external data by evaluating how our model trained on our training set performs on our test set. However, the validation approach has two major weaknesses. First, the performance of the model can be highly dependent on which few observations were selected for the test set. Second, the model is not being trained using all the available data, and not being evaluated on all the available data.\n",
    "\n",
    "A better strategy, which overcomes these weaknesses, is called *$k$-fold cross-validation* (KFCV). In KFCV, we split the data into $k$ parts called “folds.” The model is then trained using $k – 1$ folds -- combined into one training set -- and then the last fold is used as a test set. We repeat this $k$ times, each time using a different fold as the test set. The performance on the model for each of the $k$ iterations is then averaged to produce an overall measurement.\n",
    "\n",
    "In our solution, we conducted $k$-fold cross-validation using 10 folds and outputted the evaluation scores to cv_results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View score for all 10 folds\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three important points to consider when we are using KFCV. First, KFCV assumes that each observation was created independent from the other (i.e., the data is independent identically distributed [IID]). If the data is IID, it is a good idea to shuffle observations when assigning to folds. In scikit-learn we can set `shuffle=True` to perform shuffling.\n",
    "\n",
    "Second, when we are using KFCV to evaluate a classifier, it is often beneficial to have folds containing roughly the same percentage of observations from each of the different target classes (called stratified k-fold). For example, if our target vector contained gender and 80% of the observations were male, then each fold would contain 80% male and 20% female observations. In scikit-learn, we can conduct stratified k-fold cross-validation by replacing the `KFold` class with `StratifiedKFold`.\n",
    "\n",
    "Finally, when we are using validation sets or cross-validation, it is important to preprocess data based on the training set and then apply those transformations to both the training and test set. For example, when we `fit` our standardization object, `standardizer`, we calculate the mean and variance of only the training set. Then we apply that transformation (using `transform`) to both the training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create training and test sets\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    features, target, test_size=0.1, random_state=1)\n",
    "\n",
    "# Fit standardizer to training set\n",
    "standardizer.fit(features_train)\n",
    "\n",
    "# Apply to both training and test sets\n",
    "features_train_std = standardizer.transform(features_train)\n",
    "features_test_std = standardizer.transform(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason for this is because we are pretending that the test set is unknown data. If we fit both our preprocessors using observations from both training and test sets, some of the information from the test set leaks into our training set. This rule applies for any preprocessing step such as feature selection.\n",
    "\n",
    "scikit-learn’s pipeline package makes this easy to do while using cross-validation techniques. We first create a pipeline that preprocesses the data (e.g., `standardizer`) and then trains a model (logistic regression, `logit`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline\n",
    "pipeline = make_pipeline(standardizer, logit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we run KFCV using that pipeline and scikit does all the work for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do k-fold cross-validation\n",
    "cv_results = cross_val_score(pipeline, # Pipeline\n",
    "                             features, # Feature matrix\n",
    "                             target, # Target vector\n",
    "                             cv=kf, # Performance metric\n",
    "                             scoring=\"accuracy\", # Loss function\n",
    "                             n_jobs=-1) # Use all CPU cores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cross_val_score` comes with three parameters that we have not discussed that are worth noting. `cv` determines our cross-validation technique. K-fold is the most common by far, but there are others, like leave-one-out-cross-validation where the number of folds $k$ equals the number of observations. The `scoring` parameter defines our metric for success, a number of which are discussed later. Finally, `n_jobs=-1` tells scikit-learn to use every core available. For example, if your computer has four cores (a common number for laptops), then scikit-learn will use all four cores at once to speed up the operation.\n",
    "\n",
    "### See Also\n",
    "\n",
    "* [Why every statistician should know about cross-validation](http://bit.ly/2Fzhz6X)\n",
    "\n",
    "* [Cross-Validation Gone Wrong](http://bit.ly/2FzfIiw)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating a Baseline Regression Model\n",
    "\n",
    "\n",
    "### Problem\n",
    "\n",
    "You want a simple baseline regression model to compare against your model.\n",
    "### Solution\n",
    "\n",
    "Use scikit-learn’s `DummyRegressor` to create a simple model to use as a baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load data\n",
    "boston = load_boston()\n",
    "\n",
    "# create features\n",
    "features, target = boston.data, boston.target\n",
    "\n",
    "# make test and training split\n",
    "features_train, features_test, target_train, target_test = train_test_split(features, target, random_state=0)\n",
    "\n",
    "# create a dummy regressor\n",
    "dummy = DummyRegressor(strategy='mean')\n",
    "\n",
    "# \"Train\" dummy regressor\n",
    "dummy.fit(features_train, target_train)\n",
    "\n",
    "# Get R-squared score\n",
    "dummy.score(features_test, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare, we train our model and evaluate the performance score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Train simple linear regression model\n",
    "ols = LinearRegression()\n",
    "ols.fit(features_train, target_train)\n",
    "\n",
    "# Get R-squared score\n",
    "ols.score(features_test, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "`DummyRegressor` allows us to create a very simple model that we can use as a baseline to compare against our actual model. This can often be useful to simulate a “naive” existing prediction process in a product or system. For example, a product might have been originally hardcoded to assume that all new users will spend $100 in the first month, regardless of their features. If we encode that assumption into a baseline model, we are able to concretely state the benefits of using a machine learning approach.\n",
    "\n",
    "`DummyRegressor` uses the strategy parameter to set the method of making predictions, including the mean or median value in the training set. Furthermore, if we set strategy to constant and use the constant parameter, we can set the dummy regressor to predict some constant value for every observation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy regressor that predicts 20's for everything\n",
    "clf = DummyRegressor(strategy='constant', constant=20)\n",
    "clf.fit(features_train, target_train)\n",
    "\n",
    "# Evaluate score\n",
    "clf.score(features_test, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One small note regarding score. By default, score returns the coefficient of determination ($R$-squared, $R^2$) score:\n",
    "\n",
    "$$\n",
    "R^2 = 1 − \\dfrac{\\sum_i(y_i − \\hat y_i)^2} {\\sum_i(y_i − \\bar y)^2}\n",
    "$$\n",
    "\n",
    "where $y_i$ is the true value of the target observation, $\\hat y_i$ is the predicted value, and $\\bar y$ is the mean value for the target vector.  \n",
    "\n",
    "The closer $R^2$ is to 1, the more of the variance in the target vector that is explained by the features.\n",
    "\n",
    "\n",
    "## 3. Creating a Baseline Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Problem\n",
    "\n",
    "You want a simple baseline classifier to compare against your model.\n",
    "### Solution\n",
    "\n",
    "Use scikit-learn’s `DummyClassifier`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "iris = load_iris()\n",
    "\n",
    "# Create target vector and feature matrix\n",
    "features, target = iris.data, iris.target\n",
    "\n",
    "# Split into training and test set\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "features, target, random_state=0)\n",
    "\n",
    "# Create dummy classifier\n",
    "dummy = DummyClassifier(strategy='uniform', random_state=1)\n",
    "\n",
    "# \"Train\" model\n",
    "dummy.fit(features_train, target_train)\n",
    "\n",
    "# Get accuracy score\n",
    "dummy.score(features_test, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By comparing the baseline classifier to our trained classifier, we can see the improvement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create classifier\n",
    "classifier = RandomForestClassifier()\n",
    "\n",
    "# Train model\n",
    "classifier.fit(features_train, target_train)\n",
    "\n",
    "# Get accuracy score\n",
    "classifier.score(features_test, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "A common measure of a classifier’s performance is how much better it is than random guessing. scikit-learn’s `DummyClassifier` makes this comparison easy. The strategy parameter gives us a number of options for generating values. There are two particularly useful strategies. First, `stratified` makes predictions that are proportional to the training set’s target vector’s class proportions (i.e., if 20% of the observations in the training data are women, then `DummyClassifier` will predict women 20% of the time). Second, `uniform` will generate predictions uniformly at random between the different classes. For example, if 20% of observations are women and 80% are men, `uniform` will produce predictions that are 50% women and 50% men.\n",
    "\n",
    "### See Also\n",
    "\n",
    "* [scikit-learn documentation: DummyClassifier](http://bit.ly/2Fr178G)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluating Binary Classifier Predictions\n",
    "\n",
    "### Problem\n",
    "Given a trained classification model, you want to evaluate it's quality\n",
    "\n",
    "### Solution\n",
    "Use scikit-learn's `cross_val_score` to conduct cross-validation while using the `scoring` parameter to define one of a number of performance metrics, including accuracy, precision, recall, and $F_1$\n",
    "\n",
    "Accuracy is a common performance metric. It is simply the proportion of observations predicted corrected\n",
    "\n",
    "$$Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}$$\n",
    "\n",
    "where:\n",
    "\n",
    "* TP is the number of true positives. Observations that are part of the positive class (has the disease, purchased the product, etc.) and that we predicted correctly.\n",
    "\n",
    "* TN is the number of true negatives. Observations that are part of the negative class (does not have the disease, did not purchase the product, etc.) and that we predicted correctly.\n",
    "\n",
    "* FP is the number of false positives. Also called a Type I error. Observations predicted to be part of the positive class that are actually part of the negative class.\n",
    "\n",
    "* FN is the number of false negatives. Also called a Type II error. Observations predicted to be part of the negative class that are actually part of the positive class.\n",
    "\n",
    "We can measure accuracy in three-fold (the default number of folds is 5) cross-validation by setting `scoring=\"accuracy\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Generate features matrix and target vector\n",
    "X, y = make_classification(n_samples = 10000,\n",
    "                           n_features = 3,\n",
    "                           n_informative = 3,\n",
    "                           n_redundant = 0,\n",
    "                           n_classes = 2,\n",
    "                           random_state = 1)\n",
    "\n",
    "# Create logistic regression\n",
    "logit = LogisticRegression()\n",
    "\n",
    "# Cross-validate model using accuracy\n",
    "cross_val_score(logit, X, y, cv = 3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The appeal of accuracy is that it has an intuitive and plain English explanation: proportion of observations predicted correctly. However, in the real world, often our data has imbalanced classes (e.g., the 99.9% of observations are of class 1 and only 0.1% are class 2). When in the presence of imbalanced classes, accuracy suffers from a paradox where a model is highly accurate but lacks predictive power. For example, imagine we are trying to predict the presence of a very rare cancer that occurs in 0.1% of the population. After training our model, we find the accuracy is at 95%. However, 99.9% of people do not have the cancer: if we simply created a model that “predicted” that nobody had that form of cancer, our naive model would be 4.9% more accurate, but clearly is not able to *predict* anything. For this reason, we are often motivated to use other metrics like precision, recall, and the $F_1$ score.\n",
    "\n",
    "Precision is the proportion of every observation predicted to be positive that is actually positive. We can think about it as a measurement noise in our predictions—that is, when we predict something is positive, how likely we are to be right. Models with high precision are pessimistic in that they only predict an observation is of the positive class when they are very certain about it. Formally, precision is:\n",
    "\n",
    "Precision is the proportion of every observation predicted to be positive that is actually positive. We can think about it as a measurement of noise in our predictions-- that is, when we predict something is positive, how likely we are to be right. Models with high precision are pessimistic in that they only predict an observation is of the positive class when they are very certain about it. Formally, precision is:\n",
    "\n",
    "$$\\text{Precision} = \\frac{TP}{TP + FP}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validate model using precision\n",
    "cross_val_score(logit, X, y, cv = 3, scoring=\"precision\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall is the proportion of every positive observation that is truly positive. Recall measures the model’s ability to identify an observation of the positive class. Models with high recall are optimistic in that they have a low bar for predicting that an observation is in the positive class:\n",
    "\n",
    "$$ \\text{Recall} = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "It is a measure of correctness achieved in positive prediction -- that is, of observations labeled as positive, how many are actually positive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validate model using recall\n",
    "cross_val_score(logit, X, y, cv = 3,  scoring=\"recall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this is the first time you have encountered precision and recall, it is understandable if it takes you a little while to fully understand them. This is one of the downsides to accuracy; precision and recall are less intuitive. Almost always we want some kind of balance between precision and recall, and this role is filled by the $F_1$ score. The F_1$ score is the harmonic mean (a kind of average used for ratios):\n",
    "\n",
    "$$F_1 = 2 * \\frac{\\text{Precision} * \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validate model using f1\n",
    "cross_val_score(logit, X, y, cv = 3, scoring=\"f1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "As an evaluation metric, accuracy has some valuable properties, especially its simple intuition. However, better metrics often involve using some balance of precision and recall—that is, a trade-off between the optimism and pessimism of our model. F1 represents a balance between the recall and precision, where the relative contributions of both are equal.\n",
    "\n",
    "Alternatively to using `cross_val_score`, if we already have the true $y$ values and the predicted $y$ values, we can calculate metrics like accuracy and recall directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create training and test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.1,\n",
    "                                                    random_state=1)\n",
    "\n",
    "# Predict values for training target vector\n",
    "y_hat = logit.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_score(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  See Also\n",
    "* [Accuracy paradox](https://en.wikipedia.org/wiki/Accuracy_paradox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluating Binary Classifier Thresholds\n",
    "\n",
    "### Problem\n",
    "\n",
    "You want to evaluate a binary classifier and various probability thresholds.\n",
    "### Solution\n",
    "\n",
    "The Receiving Operating Characteristic (ROC) curve is a common method for evaluating the quality of a binary classifier. ROC compares the presence of true positives and false positives at every probability threshold (i.e., the probability at which an observation is predicted to be a class). By plotting the ROC curve, we can see how the model performs. A classifier that predicts every observation correctly would look like the solid light gray line in the following chart, going straight up to the top immediately. A classifier that predicts at random will appear as the diagonal line. The better the model, the closer it is to the solid line. In scikit-learn, we can use `roc_curve` to calculate the true and false positives at each threshold, then plot them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create feature matrix and target vector\n",
    "features, target = make_classification(n_samples=10000,\n",
    "                                       n_features=10,\n",
    "                                       n_classes=2,\n",
    "                                       n_informative=3,\n",
    "                                       random_state=3)\n",
    "\n",
    "# Split into training and test sets\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    features, target, test_size=0.1, random_state=1)\n",
    "\n",
    "# Create classifier\n",
    "logit = LogisticRegression()\n",
    "\n",
    "# Train model\n",
    "logit.fit(features_train, target_train)\n",
    "\n",
    "# Get predicted probabilities\n",
    "target_probabilities = logit.predict_proba(features_test)[:,1]\n",
    "\n",
    "# Create true and false positive rates\n",
    "false_positive_rate, true_positive_rate, threshold = roc_curve(target_test,\n",
    "                                                               target_probabilities)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.title(\"Receiver Operating Characteristic\")\n",
    "plt.plot(false_positive_rate, true_positive_rate)\n",
    "plt.plot([0, 1], ls=\"--\")\n",
    "plt.plot([0, 0], [1, 0] , c=\".7\"), plt.plot([1, 1] , c=\".7\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "Up until now we have only examined models based on the values they predict. However, in many learning algorithms those predicted values are based off of probability estimates. That is, each observation is given an explicit probability of belonging in each class. In our solution, we can use `predict_proba` to see the predicted probabilities for the first observation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted probabilities\n",
    "logit.predict_proba(features_test)[0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the classes using `classes_`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the first observation has an ~87% chance of being in the negative class (0) and a 13% chance of being in the positive class (1). By default, scikit-learn predicts an observation is part of the positive class if the probability is greater than 0.5 (called the threshold). However, instead of a middle ground, we will often want to explicitly bias our model to use a different threshold for substantive reasons. For example, if a false positive is very costly to our company, we might prefer a model that has a high probability threshold. We fail to predict some positives, but when an observation is predicted to be positive, we can be very confident that the prediction is correct. This trade-off is represented in the true positive rate (TPR) and the false positive rate (FPR). The true positive rate is the number of observations correctly predicted true divided by all true positive observations:\n",
    "\n",
    "$$ \\text{TPR} = \\frac{\\text{True Positives}}{\\text{True Positives}+\\text{False Negatives}}$$\n",
    "\n",
    "The false positive rate is the number of incorrectly predicted positives divided by all true negative observations:\n",
    "\n",
    "$$ \\text{FPR} = \\frac{\\text{False Positives}}{\\text{False Positives} + \\text{True Negatives}}$$\n",
    "\n",
    "The ROC curve represents the respective TPR and FPR for every probability threshold. For example, in our solution a threshold of roughly 0.50 has a TPR of 0.81 and an FPR of 0.15:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Threshold:\", threshold[116])\n",
    "print(\"True Positive Rate:\", true_positive_rate[116])\n",
    "print(\"False Positive Rate:\", false_positive_rate[116])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if we increase the threshold to ~80% (i.e., increase how certain the model has to be before it predicts an observation as positive) the TPR drops significantly but so does the FPR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Threshold:\", threshold[45])\n",
    "print(\"True Positive Rate:\", true_positive_rate[45])\n",
    "print(\"False Positive Rate:\", false_positive_rate[45])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because our higher requirement for being predicted to be in the positive class has made the model not identify a number of positive observations (the lower TPR), but also reduce the noise from negative observations being predicted as positive (the lower FPR).\n",
    "\n",
    "In addition to being able to visualize the trade-off between TPR and FPR, the ROC curve can also be used as a general metric for a model. The better a model is, the higher the curve and thus the greater the area under the curve. For this reason, it is common to calculate the area under the ROC curve (AUCROC) to judge the overall quality of a model at all possible thresholds. The closer the AUCROC is to 1, the better the model. In scikit-learn we can calculate the AUCROC using `roc_auc_score`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate area under curve\n",
    "roc_auc_score(target_test, target_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See Also\n",
    "* [ROC Curves in Python and R](https://community.alteryx.com/t5/Data-Science-Blog/ROC-Curves-in-Python-and-R/ba-p/138430)\n",
    "* [The Area Under a ROC Curve](http://gim.unmc.edu/dxtests/roc3.htm)\n",
    "\n",
    "## 6. Evaluating Multiclass Classifier Predictions\n",
    "\n",
    "\n",
    "### Problem\n",
    "\n",
    "You have a model that predicts three or more classes and want to evaluate its performance.\n",
    "### Solution\n",
    "\n",
    "Use cross-validation with an evaluation metric capable of handling more than two classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Generate features matrix and target vector\n",
    "features, target = make_classification(n_samples = 10000,\n",
    "                           n_features = 3,\n",
    "                           n_informative = 3,\n",
    "                           n_redundant = 0,\n",
    "                           n_classes = 3,\n",
    "                           random_state = 1)\n",
    "\n",
    "# Create logistic regression\n",
    "logit = LogisticRegression()\n",
    "\n",
    "# Cross-validate model using accuracy\n",
    "cross_val_score(logit, features, target, cv = 3, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "When we have balanced classes (e.g., a roughly equal number of observations in each class of the target vector), accuracy is -- just like in the binary class setting -- a simple and interpretable choice for an evaluation metric. Accuracy is the number of correct predictions divided by the number of observations and works just as well in the multiclass as binary setting. However, when we have imbalanced classes (a common scenario), we should be inclined to use other evaluation metrics.\n",
    "\n",
    "Many of scikit-learn’s built-in metrics are for evaluating binary classifiers. However, many of these metrics can be extended for use when we have more than two classes. Precision, recall, and $F_1$ scores are useful metrics that we have already covered in detail in previous exercises. While all of them were originally designed for binary classifiers, we can apply them to multiclass settings by treating our data as a set of binary classes. Doing so enables us to apply the metrics to each class as if it were the only class in the data, and then aggregate the evaluation scores for all the classes by averaging them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validate model using macro averaged F1 score\n",
    "cross_val_score(logit, features, target,cv = 3, scoring='f1_macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, `_macro` refers to the method used to average the evaluation scores from the classes:\n",
    "\n",
    "`macro`\n",
    "\n",
    "    Calculate mean of metric scores for each class, weighting each class equally.\n",
    "`weighted`\n",
    "\n",
    "    Calculate mean of metric scores for each class, weighting each class proportional to its size in the data.\n",
    "`micro`\n",
    "\n",
    "    Calculate mean of metric scores for each observation-class combination.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizing a Classifier's Performance\n",
    "\n",
    "### Problem\n",
    "\n",
    "Given predicted classes and true classes of the test data, you want to visually compare the model’s quality.\n",
    "### Solution\n",
    "\n",
    "Use a confusion matrix, which compares predicted classes and true classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Create feature matrix\n",
    "features = iris.data\n",
    "\n",
    "# Create target vector\n",
    "target = iris.target\n",
    "\n",
    "# Create list of target class names\n",
    "class_names = iris.target_names\n",
    "\n",
    "# Create training and test set\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    features, target, random_state=1)\n",
    "\n",
    "# Create logistic regression\n",
    "classifier = LogisticRegression(max_iter = 150)\n",
    "\n",
    "# Train model and make predictions\n",
    "target_predicted = classifier.fit(features_train,\n",
    "    target_train).predict(features_test)\n",
    "\n",
    "# Create confusion matrix\n",
    "matrix = confusion_matrix(target_test, target_predicted)\n",
    "\n",
    "# Create pandas dataframe\n",
    "dataframe = pd.DataFrame(matrix, index=class_names, columns=class_names)\n",
    "\n",
    "# Create heatmap\n",
    "sns.heatmap(dataframe, annot=True, cbar=None, cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix\"), plt.tight_layout()\n",
    "plt.ylabel(\"True Class\"), plt.xlabel(\"Predicted Class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Confusion matrices are an easy, effective visualization of a classifier’s performance. One of the major benefits of confusion matrices is their interpretability. Each column of the matrix (often visualized as a heatmap) represents predicted classes, while every row shows true classes. The end result is that every cell is one possible combination of predict and true classes. This is probably best explained using an example. In the solution, the top-left cell is the number of observations predicted to be Iris setosa (indicated by the column) that are actually Iris setosa (indicated by the row). This means the models accurately predicted all Iris setosa flowers. However, the model does not do as well at predicting Iris virginica. The bottom-right cell indicates that the model successfully predicted nine observations were Iris virginica, but (looking one cell up) predicted six flowers to be viriginica that were actually Iris versicolor.\n",
    "\n",
    "There are three things worth noting about confusion matrices. First, a perfect model will have values along the diagonal and zeros everywhere else. A bad model will look like the observation counts will be spread evenly around cells. Second, a confusion matrix lets us see not only where the model was wrong, but also how it was wrong. That is, we can look at patterns of misclassification. For example, our model had an easy time differentiating Iris virginica and Iris setosa, but a much more difficult time classifying Iris virginica and Iris versicolor. Finally, confusion matrices work with any number of classes (although if we had one million classes in our target vector, the confusion matrix visualization might be difficult to read).\n",
    "\n",
    "### See Also\n",
    "\n",
    "* [Confusion Matrix](https://en.wikipedia.org/wiki/Confusion_matrix)\n",
    "\n",
    "* [scikit-learn documentation: Confusion Matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)\n",
    "\n",
    "\n",
    "\n",
    "## 8. Evaluating Regression Models\n",
    "\n",
    "### Problem\n",
    "\n",
    "You want to evaluate the performance of a regression model.\n",
    "### Solution\n",
    "\n",
    "Use mean squared error (MSE):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Generate features matrix, target vector\n",
    "features, target = make_regression(n_samples = 100,\n",
    "                                   n_features = 3,\n",
    "                                   n_informative = 3,\n",
    "                                   n_targets = 1,\n",
    "                                   noise = 50,\n",
    "                                   coef = False,\n",
    "                                   random_state = 1)\n",
    "\n",
    "# Create a linear regression object\n",
    "ols = LinearRegression()\n",
    "\n",
    "# Cross-validate the linear regression using (negative) MSE\n",
    "cross_val_score(ols, features, target, cv = 3, scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common regression metric is the coefficient of determination, $R^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validate the linear regression using R-squared\n",
    "cross_val_score(ols, features, target, cv = 3, scoring='r2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "MSE is one of the most common evaluation metrics for regression models. Formally, MSE is:\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\dfrac{1}{n} \\sum_i^n = (\\hat y_i − y_i)^2\n",
    "$$\n",
    "\n",
    "where $n$ is the number of observations, $y_i$ is the true value of the target we are trying to predict for observation $i$, and $\\hat y_i$ is the model’s predicted value for $y_i$. MSE is a measurement of the squared sum of all distances between predicted and true values. The higher the value of MSE, the greater the total squared error and thus the worse the model. There are a number of mathematical benefits to squaring the error term, including that it forces all error values to be positive, but one often unrealized implication is that squaring penalizes a few large errors more than many small errors, even if the absolute value of the errors is the same. For example, imagine two models, A and B, each with two observations:\n",
    "\n",
    "* Model A has errors of 0 and 10 and thus its MSE is $0^2 + 10^2 = 100$.\n",
    "\n",
    "* Model B has two errors of 5 each, and thus its MSE is $5^2 + 5^2 = 50$.\n",
    "\n",
    "Both models have the same total error, 10; however, MSE would consider Model A (MSE = 100) worse than Model B (MSE = 50). In practice this implication is rarely an issue (and indeed can be theoretically beneficial) and MSE works perfectly fine as an evaluation metric.\n",
    "\n",
    "One important note: by default in scikit-learn arguments of the scoring parameter assume that higher values are better than lower values. However, this is not the case for MSE, where higher values mean a worse model. For this reason, scikit-learn looks at the negative MSE using the `neg_mean_squared_error` argument.\n",
    "\n",
    "A common alternative regression evaluation metric is $R^2$, which measures the amount of variance in the target vector that is explained by the model:\n",
    "\n",
    "$$\n",
    "R^2 = 1 − \\dfrac{\\sum_{i=1}^n(y_i − \\hat y_i)^2}{\\sum^n_{i=1} (y_i − \\bar y)^2}\n",
    "$$\n",
    "\n",
    "where $y_i$ is the true target value of the $i$th observation, $\\hat y_i$ is the predicted value for the $i$th observation, and $\\bar y$ is the mean value of the target vector. The closer to 1.0, the better the model.\n",
    "\n",
    "\n",
    "### See Also\n",
    "\n",
    "* [Mean squared error](https://en.wikipedia.org/wiki/Mean_squared_error)\n",
    "\n",
    "* [Coefficient of determination](https://en.wikipedia.org/wiki/Coefficient_of_determination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Creating a Custom Evaluation Metric\n",
    "### Problem\n",
    "\n",
    "You want to evaluate a model using a metric you created.\n",
    "### Solution\n",
    "\n",
    "Create the metric as a function and convert it into a scorer function using scikit-learn’s `make_scorer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from sklearn.metrics import make_scorer, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Generate features matrix and target vector\n",
    "features, target = make_regression(n_samples = 100,\n",
    "                                   n_features = 3,\n",
    "                                   random_state = 1)\n",
    "\n",
    "# Create training set and test set\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "     features, target, test_size=0.10, random_state=1)\n",
    "\n",
    "# Create custom metric\n",
    "def custom_metric(target_test, target_predicted):\n",
    "    # Calculate r-squared score\n",
    "    r2 = r2_score(target_test, target_predicted)\n",
    "    # Return r-squared score\n",
    "    return r2\n",
    "\n",
    "# Make scorer and define that higher scores are better\n",
    "score = make_scorer(custom_metric, greater_is_better=True)\n",
    "\n",
    "# Create ridge regression object\n",
    "classifier = Ridge()\n",
    "\n",
    "# Train ridge regression model\n",
    "model = classifier.fit(features_train, target_train)\n",
    "\n",
    "# Apply custom scorer\n",
    "score(model, features_test, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "While scikit-learn has a number of built-in metrics for evaluating model performance, it is often useful to define our own metrics. scikit-learn makes this easy using `make_scorer`. First, we define a function that takes in two arguments -- the ground truth target vector and our predicted values -- and outputs some score. Second, we use `make_scorer` to create a scorer object, making sure to specify whether higher or lower scores are desirable (using the `greater_is_better` parameter).\n",
    "\n",
    "The custom metric in the solution (custom_metric) is a toy example since it simply wraps a built-in metric for calculating the $R^2$ score. In a real-world situation, we would replace the `custom_metric` function with whatever custom metric we wanted. However, we can see that the custom metric that calculates $R^2$ does work by comparing the results to scikit-learn’s `r2_score` built-in method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict values\n",
    "target_predicted = model.predict(features_test)\n",
    "\n",
    "# Calculate r-squared score\n",
    "r2_score(target_test, target_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### See Also\n",
    "\n",
    "* [scikit-learn documentation: make_scorer](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html#sklearn.metrics.make_scorer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualizing the Effect of Training Set Size\n",
    "\n",
    "### Problem\n",
    "\n",
    "You want to evaluate the effect of the number of observations in your training set on some metric (accuracy, $F_1$, etc.).\n",
    "### Solution\n",
    "\n",
    "Plot the learning curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "# Load data\n",
    "digits = load_digits()\n",
    "\n",
    "# Create feature matrix and target vector\n",
    "features, target = digits.data, digits.target\n",
    "\n",
    "# Create CV training and test scores for various training set sizes\n",
    "train_sizes, train_scores, test_scores = learning_curve(# Classifier\n",
    "                                                        RandomForestClassifier(),\n",
    "                                                        # Feature matrix\n",
    "                                                        features,\n",
    "                                                        # Target vector\n",
    "                                                        target,\n",
    "                                                        # Number of folds\n",
    "                                                        cv=10,\n",
    "                                                        # Performance metric\n",
    "                                                        scoring='accuracy',\n",
    "                                                        # Use all computer cores\n",
    "                                                        n_jobs=-1,\n",
    "                                                        # Sizes of 50\n",
    "                                                        # training set\n",
    "                                                       train_sizes=np.linspace(\n",
    "                                                       0.01,\n",
    "                                                       1.0,\n",
    "                                                       50))\n",
    "\n",
    "# Create means and standard deviations of training set scores\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "\n",
    "# Create means and standard deviations of test set scores\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Draw lines\n",
    "plt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\n",
    "plt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n",
    "\n",
    "# Draw bands\n",
    "plt.fill_between(train_sizes, train_mean - train_std,\n",
    "                 train_mean + train_std, color=\"#DDDDDD\")\n",
    "plt.fill_between(train_sizes, test_mean - test_std,\n",
    "                 test_mean + test_std, color=\"#DDDDDD\")\n",
    "\n",
    "# Create plot\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"),\n",
    "plt.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Learning curves visualize the performance (e.g., accuracy, recall) of a model on the training set and during cross-validation as the number of observations in the training set increases. They are commonly used to determine if our learning algorithms would benefit from gathering additional training data.\n",
    "\n",
    "In our solution, we plot the accuracy of a random forest classifier at 50 different training set sizes ranging from 1% of observations to 100%. The increasing accuracy score of the cross-validated models tell us that we would likely benefit from additional observations (although in practice this might not be feasible).\n",
    "\n",
    "### See Also\n",
    "\n",
    "* [scikit-learn documentation: Learning Curve](http://scikit-learn.org/stable/modules/learning_curve.html#learning-curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Creating a Text Report of Evaluation Metrics\n",
    "\n",
    "### Problem\n",
    "\n",
    "You want a quick description of a classifier’s performance.\n",
    "### Solution\n",
    "\n",
    "Use scikit-learn’s `classification_report`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load data\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Create feature matrix\n",
    "features = iris.data\n",
    "\n",
    "# Create target vector\n",
    "target = iris.target\n",
    "\n",
    "# Create list of target class names\n",
    "class_names = iris.target_names\n",
    "\n",
    "# Create training and test set\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    features, target, random_state=1)\n",
    "\n",
    "# Create logistic regression\n",
    "classifier = LogisticRegression(max_iter = 250)\n",
    "\n",
    "# Train model and make predictions\n",
    "model = classifier.fit(features_train, target_train)\n",
    "target_predicted = model.predict(features_test)\n",
    "\n",
    "# Create a classification report\n",
    "print(classification_report(target_test,\n",
    "                            target_predicted,\n",
    "                            target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "`classification_report` provides a quick means for us to see some common evaluation metrics, including precision, recall, and $F_1$-score. Support refers to the number of observations in each class.\n",
    "\n",
    "### See Also\n",
    "\n",
    "* [Precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Visualizing the Effect of Hyperparameter Values\n",
    "\n",
    "### Problem\n",
    "\n",
    "You want to understand how the performance of a model changes as the value of some hyperparameter changes.\n",
    "### Solution\n",
    "\n",
    "Plot the validation curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "# Load data\n",
    "digits = load_digits()\n",
    "\n",
    "# Create feature matrix and target vector\n",
    "features, target = digits.data, digits.target\n",
    "\n",
    "# Create range of values for parameter\n",
    "param_range = np.arange(1, 250, 2)\n",
    "\n",
    "# Calculate accuracy on training and test set using range of parameter values\n",
    "train_scores, test_scores = validation_curve(\n",
    "    # Classifier\n",
    "    RandomForestClassifier(),\n",
    "    # Feature matrix\n",
    "    features,\n",
    "    # Target vector\n",
    "    target,\n",
    "    # Hyperparameter to examine\n",
    "    param_name=\"n_estimators\",\n",
    "    # Range of hyperparameter's values\n",
    "    param_range=param_range,\n",
    "    # Number of folds\n",
    "    cv=3,\n",
    "    # Performance metric\n",
    "    scoring=\"accuracy\",\n",
    "    # Use all computer cores\n",
    "    n_jobs=-1)\n",
    "\n",
    "# Calculate mean and standard deviation for training set scores\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "\n",
    "# Calculate mean and standard deviation for test set scores\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Plot mean accuracy scores for training and test sets\n",
    "plt.plot(param_range, train_mean, label=\"Training score\", color=\"black\")\n",
    "plt.plot(param_range, test_mean, label=\"Cross-validation score\", color=\"dimgrey\")\n",
    "\n",
    "# Plot accurancy bands for training and test sets\n",
    "plt.fill_between(param_range, train_mean - train_std,\n",
    "                 train_mean + train_std, color=\"gray\")\n",
    "plt.fill_between(param_range, test_mean - test_std,\n",
    "                 test_mean + test_std, color=\"gainsboro\")\n",
    "\n",
    "# Create plot\n",
    "plt.title(\"Validation Curve With Random Forest\")\n",
    "plt.xlabel(\"Number Of Trees\")\n",
    "plt.ylabel(\"Accuracy Score\")\n",
    "plt.tight_layout()\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Most training algorithms (including many covered in this book) contain hyperparameters that must be chosen before the training process begins. For example, a random forest classifier creates a “forest” of decision trees, each of which votes on the predicted class of an observation. One hyperparameter in random forest classifiers is the number of trees in the forest. Most often hyperparameter values are selected during model selection. However, it is occasionally useful to visualize how model performance changes as the hyperparameter value changes. In our solution, we plot the changes in accuracy for a random forest classifier for the training set and during cross-validation as the number of trees increases. When we have a small number of trees, both the training and cross-validation score are low, suggesting the model is underfitted. As the number of trees increases to 250, the accuracy of both levels off, suggesting there is probably not much value in the computational cost of training a massive forest.\n",
    "\n",
    "In scikit-learn, we can calculate the validation curve using `validation_curve`, which contains three important parameters:\n",
    "\n",
    "* `param_name` is the name of the hyperparameter to vary.\n",
    "\n",
    "* `param_range` is the value of the hyperparameter to use.\n",
    "\n",
    "* `scoring` is the evaluation metric used to judge to model.\n",
    "\n",
    "### See Also\n",
    "\n",
    "* [scikit-learn documentation: Validation Curve](https://scikit-learn.org/stable/modules/learning_curve.html#validation-curve)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
