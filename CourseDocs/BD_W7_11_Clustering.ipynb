{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning: Clustering\n",
    "\n",
    "## Introduction\n",
    "\n",
    "We have looked at supervised machine learning -- where we have access to both the features and the target. This is, unfortunately, not always the case. Frequently, we run into situations where we only know the features. For example, imagine we have records of sales from a grocery store and we want to break up sales by whether or not the shopper is a member of a discount club. This would be impossible using supervised learning because we don’t have a target to train and evaluate our models. However, there is another option: unsupervised learning. If the behavior of discount club members and nonmembers in the grocery store is actually disparate, then the average difference in behavior between two members will be smaller than the average difference in behavior between a member and nonmember shopper. Put another way, there will be two clusters of observations.\n",
    "\n",
    "The goal of clustering algorithms is to identify those latent groupings of observations, which if done well, allow us to predict the class of observations even without a target vector. There are many clustering algorithms and they have a wide variety of approaches to identifying the clusters in data. In this module, we will cover a selection of clustering algorithms using scikit-learn and how to use them in practice.\n",
    "\n",
    "### See Also\n",
    "[Scikit-learn Clustering Overview](https://scikit-learn.org/stable/modules/clustering.html#clustering)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Clustering Using K-Means\n",
    "\n",
    "### Problem\n",
    "\n",
    "You want to group observations into $k$ groups.\n",
    "### Solution\n",
    "\n",
    "Use $k$-means clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Load data\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "features_std = scaler.fit_transform(features)\n",
    "\n",
    "# Create k-mean object\n",
    "cluster = KMeans(n_clusters=3, random_state=0)\n",
    "\n",
    "# Train model\n",
    "model = cluster.fit(features_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "$k$-means clustering is one of the most common clustering techniques. In $k$-means clustering, the algorithm attempts to group observations into $k$ groups, with each group having roughly equal variance. The number of groups, $k$, is specified by the user as a hyperparameter. Specifically, in $k$-means:\n",
    "\n",
    "1. $k$ cluster “center” points are created at random locations.\n",
    "\n",
    "2. For each observation:\n",
    "\n",
    "* The distance between each observation and the $k$ center points is calculated.\n",
    "\n",
    "* The observation is assigned to the cluster of the nearest center point.\n",
    "\n",
    "3. The center points are moved to the means (i.e., centers) of their respective clusters.\n",
    "\n",
    "4. Steps 2 and 3 are repeated until no observation changes in cluster membership.\n",
    "\n",
    "At this point the algorithm is considered converged and stops.\n",
    "\n",
    "It is important to note three things about $k$-means. First, $k$-means clustering assumes the clusters are convex shaped (e.g., a circle, a sphere). Second, all features are equally scaled. In our solution, we standardized the features to meet this assumption. Third, the groups are balanced (i.e., have roughly the same number of observations). If we suspect that we cannot meet these assumptions, we might try other clustering approaches.\n",
    "\n",
    "In scikit-learn, $k$-means clustering is implemented in the `KMeans` class. The most important parameter is `n_clusters`, which sets the number of clusters $k$. In some situations, the nature of the data will determine the value for $k$ (e.g., data on a school’s students will have one cluster per grade), but often we don’t know the number of clusters. In these cases, we will want to select $k$ based on using some criteria. For example, silhouette coefficients measure the similarity within clusters compared with the similarity between clusters.\n",
    "\n",
    "In our solution, we cheated a little and used the Iris flower data, in which we know there are three classes. Therefore, we set $k = 3$. We can use `labels_` to see the predicted classes of each observation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View predict class\n",
    "model.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we compare this to the observation’s true class we can see that despite the difference in class labels (i.e., 0, 1, and 2), $k$-means did reasonably well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View true class\n",
    "iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, as you might imagine, the performance of $k$-means drops considerably, even critically, if we select the wrong number of clusters.\n",
    "\n",
    "Finally, as with other scikit-learn methods we can use the trained cluster to predict the value of new observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new observation\n",
    "new_observation = [[0.8, 0.8, 0.8, 0.8]]\n",
    "\n",
    "# Predict observation's cluster\n",
    "model.predict(new_observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observation is predicted to belong to the cluster whose center point is closest. We can even use `cluster_centers_` to see those center points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View cluster centers\n",
    "model.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See Also\n",
    "* [Introduction to K-means Clustering](https://blog.floydhub.com/introduction-to-k-means-clustering-in-python-with-scikit-learn/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Speeding Up K-Means Clustering\n",
    "\n",
    "### Problem\n",
    "\n",
    "You want to group observations into $k$ groups, but $k$-means takes too long.\n",
    "### Solution\n",
    "\n",
    "Use mini-batch $k$-means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "# Load data\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "features_std = scaler.fit_transform(features)\n",
    "\n",
    "# Create k-mean object\n",
    "cluster = MiniBatchKMeans(n_clusters=3, random_state=0, batch_size=100)\n",
    "\n",
    "# Train model\n",
    "model = cluster.fit(features_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Mini-batch $k$-means works similarly to the $k$-means algorithm. Without going into too much detail, the difference is that in mini-batch $k$-means the most computationally costly step is conducted on only a random sample of observations as opposed to all observations. This approach can significantly reduce the time required for the algorithm to find convergence (i.e., fit the data) with only a small cost in quality.\n",
    "\n",
    "`MiniBatchKMeans` works similarly to `KMeans`, with one significant difference: the `batch_size` parameter. `batch_size` controls the number of randomly selected observations in each batch. The larger the size of the batch, the more computationally costly the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clustering Using Meanshift\n",
    "\n",
    "### Problem\n",
    "\n",
    "You want to group observations without assuming the number of clusters or their shape.\n",
    "### Solution\n",
    "\n",
    "Use meanshift clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import MeanShift\n",
    "\n",
    "# Load data\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "features_std = scaler.fit_transform(features)\n",
    "\n",
    "# Create meanshift object\n",
    "cluster = MeanShift(n_jobs=-1)\n",
    "\n",
    "# Train model\n",
    "model = cluster.fit(features_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "One of the disadvantages of $k$-means clustering we discussed previously is that we needed to set the number of clusters, $k$, prior to training, and the method made assumptions about the shape of the clusters. One clustering algorithm without these limitations is meanshift.\n",
    "\n",
    "Meanshift is a simple concept, but somewhat difficult to explain. Therefore, an analogy might be the best approach. Imagine a very foggy football field (i.e., a two-dimensional feature space) with 100 people standing on it (i.e., our observations). Because it is foggy, a person can only see a short distance. Every minute each person looks around and takes a step in the direction of the most people they can see. As time goes on, people start to group up as they repeatedly take steps toward larger and larger crowds. The end result is clusters of people around the field. People are assigned to the clusters in which they end up.\n",
    "\n",
    "scikit-learn’s actual implementation of meanshift, `MeanShift`, is more complex but follows the same basic logic. MeanShift has two important parameters we should be aware of. First, bandwidth sets the radius of the area (i.e., kernel) an observation uses to determine the direction to shift. In our analogy, bandwidth was how far a person could see through the fog. We can set this parameter manually, but by default a reasonable bandwidth is estimated automatically (with a significant increase in computational cost). Second, sometimes in meanshift there are no other observations within an observation’s kernel. That is, a person on our football field cannot see a single other person. By default, MeanShift assigns all these “orphan” observations to the kernel of the nearest observation. However, if we want to leave out these orphans, we can set cluster_all=False wherein orphan observations are given the label of -1.\n",
    "\n",
    "### See Also\n",
    "\n",
    "* [The mean shift clustering algorithm, EFAVDB](https://www.efavdb.com/mean-shift)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clustering Using DBSCAN\n",
    "\n",
    "### Problem\n",
    "\n",
    "You want to group observations into clusters of high density.\n",
    "### Solution\n",
    "\n",
    "Use DBSCAN clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Load data\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "features_std = scaler.fit_transform(features)\n",
    "\n",
    "# Create DBSCAN object\n",
    "cluster = DBSCAN(n_jobs=-1)\n",
    "\n",
    "# Train model\n",
    "model = cluster.fit(features_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "DBSCAN is motivated by the idea that clusters will be areas where many observations are densely packed together and makes no assumptions of cluster shape. Specifically, in DBSCAN:\n",
    "\n",
    "1. A random observation, $x_i$, is chosen.\n",
    "\n",
    "2. If $x_i$ has a minimum number of close neighbors, we consider it to be part of a cluster.\n",
    "\n",
    "3. Step 2 is repeated recursively for all of $x_i$’s neighbors, then neighbor’s neighbor, and so on. These are the cluster’s core observations.\n",
    "\n",
    "4. Once step 3 runs out of nearby observations, a new random point is chosen (i.e., restarting step 1).\n",
    "\n",
    "Once this is complete, we have a set of core observations for a number of clusters. Finally, any observation close to a cluster but not a core sample is considered part of a cluster, while any observation not close to the cluster is labeled an outlier.\n",
    "\n",
    "DBSCAN has three main parameters to set:\n",
    "\n",
    "`eps`\n",
    "The maximum distance from an observation for another observation to be considered its neighbor.\n",
    "\n",
    "`min_samples`\n",
    "The minimum number of observations less than eps distance from an observation for it to be considered a core observation.\n",
    "\n",
    "`metric`\n",
    "The distance metric used by eps—for example, minkowski or euclidean (note that if Minkowski distance is used, the parameter $p$ can be used to set the power of the Minkowski metric).\n",
    "\n",
    "If we look at the clusters in our training data we can see two clusters have been identified, 0 and 1, while outlier observations are labeled -1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show cluster membership\n",
    "model.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See Also\n",
    "\n",
    "* [DBSCAN, Wikipedia](https://en.wikipedia.org/wiki/DBSCAN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Clustering using Agglomerative Clustering\n",
    "\n",
    "### Problem\n",
    "\n",
    "You want to group observations using a hierarchy of clusters.\n",
    "### Solution\n",
    "\n",
    "Use agglomerative clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Load data\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "features_std = scaler.fit_transform(features)\n",
    "\n",
    "# Create agglomerative clustering object\n",
    "cluster = AgglomerativeClustering(n_clusters=3)\n",
    "\n",
    "# Train model\n",
    "model = cluster.fit(features_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Discussion\n",
    "\n",
    "Agglomerative clustering is a powerful, flexible hierarchical clustering algorithm. In agglomerative clustering, all observations start as their own clusters. Next, clusters meeting some criteria are merged together. This process is repeated, growing clusters until some end point is reached. In scikit-learn, `AgglomerativeClustering` uses the linkage parameter to determine the merging strategy to minimize the following:\n",
    "\n",
    "1. Variance of merged clusters (`ward`)\n",
    "\n",
    "2. Average distance between observations from pairs of clusters (`average`)\n",
    "\n",
    "3. Maximum distance between observations from pairs of clusters (`complete`)\n",
    "\n",
    "Two other parameters are useful to know. First, the affinity parameter determines the distance metric used for `linkage` (`minkowski`, `euclidean`, etc.). Second, `n_clusters` sets the number of clusters the clustering algorithm will attempt to find. That is, clusters are successively merged until there are only `n_clusters` remaining.\n",
    "\n",
    "As with other clustering algorithms we have covered, we can use `labels_` to see the cluster in which every observation is assigned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show cluster membership\n",
    "model.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluating Clustering Models\n",
    "### Problem\n",
    "\n",
    "You have used an unsupervised learning algorithm to cluster your data. Now you want to know how well it did.\n",
    "### Solution\n",
    "\n",
    "The short answer is that you probably can’t, at least not in the way you want.\n",
    "\n",
    "That said, one option is to evaluate clustering using silhouette coefficients, which measure the quality of the clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Generate feature matrix\n",
    "features, _ = make_blobs(n_samples = 1000,\n",
    "                         n_features = 10,\n",
    "                         centers = 2,\n",
    "                         cluster_std = 0.5,\n",
    "                         shuffle = True,\n",
    "                         random_state = 1)\n",
    "\n",
    "# Cluster data using k-means to predict classes\n",
    "model = KMeans(n_clusters=2, random_state=1).fit(features)\n",
    "\n",
    "# Get predicted classes\n",
    "target_predicted = model.labels_\n",
    "\n",
    "# Evaluate model\n",
    "silhouette_score(features, target_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Supervised model evaluation compares predictions (e.g., classes or quantitative values) with the corresponding true values in the target vector. However, the most common motivation for using clustering methods is that your data doesn’t \n",
    "have a target vector. There are a number of clustering evaluation metrics that require a target vector, but again, using unsupervised learning approaches like clustering when you have a target vector available to you is probably handicapping yourself unnecessarily.\n",
    "\n",
    "While we cannot evaluate predictions versus true values if we don’t have a target vector, we can evaluate the nature of the clusters themselves. Intuitively, we can imagine “good” clusters having very small distances between observations in the same cluster (i.e., dense clusters) and large distances between the different clusters (i.e., well-separated clusters). Silhouette coefficients provide a single value measuring both traits. Formally, the $i^\\text{th}$ observation’s silhouette coefficient is:\n",
    "\n",
    "$$\n",
    "s_i = \\dfrac{b_i−a_i} {\\max(a_i,b_i)}\n",
    "$$\n",
    "\n",
    "where $s_i$ is the silhouette coefficient for observation $i$, $a_i$ is the mean distance between $i$ and all observations of the same class, and $b_i$ is the mean distance between $i$ and all observations from the closest cluster of a different class. The value returned by `silhouette_score` is the mean silhouette coefficient for all observations. Silhouette coefficients range between –1 and 1, with 1 indicating dense, well-separated clusters.\n",
    "\n",
    "### See Also\n",
    "\n",
    "* [scikit-learn documentation: silhouette_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html#sklearn.metrics.silhouette_score)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
